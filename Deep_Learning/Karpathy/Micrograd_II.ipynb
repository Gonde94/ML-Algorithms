{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building out a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The constructor __init__ takes in number of inputs into the neuron as an argument. It creates a weight for each input, which is a random number between -1 and 1. It also creates a bias (controlling the over all \"trigger-happiness\" of the neuron) in the same range.\n",
    "* __call__ returns an output of the forward pass. We zip together w1 and x1 and multiply them together pairwise. We sum the results and add the bias (added as the second argument in the sum function). Lastly, we pass this through an activation function - in this case relu. Re-running the cell returns a different output each time as the weights and biases are randomly initialised each time.\n",
    "\n",
    "So, below is just one neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.03800872160737, grad=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(np.random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(np.random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # w * x + b. \n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.relu()\n",
    "        return out\n",
    "\n",
    "x = [2.0, 3.0]\n",
    "n = Neuron(2)\n",
    "n(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer is made of multiple neurons. The neurons in that layer are not connected to each other, but are connected to each of the inputs. We are defining a Layer class, which is literally just a list of neurons.\n",
    "* In the constructor, nin is number of neurons in the layer, nout is the number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0, grad=0),\n",
       " Value(data=0, grad=0),\n",
       " Value(data=4.783731524406042, grad=0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "x = [2.0, 3.0]\n",
    "n = Layer(2, 3)\n",
    "n(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=1.4670991636055768, grad=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So if we want to implement a network with 3 input neurons, 2 layers of 4 neurons and 1 output neuron:\n",
    "\n",
    "inputs = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4,4,1])\n",
    "n(x)\n",
    "\n",
    "# And boom, there is a forward pass through the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another example\n",
    "\n",
    "Create a simple dataset. There are 4 examples with 3 inputs for each. We also have a list of desired targets. So we want the weights to be adjusted so that the first output = 1.0, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=2.031693635563312, grad=0),\n",
       " Value(data=0.7530379027357996, grad=0),\n",
       " Value(data=0, grad=0),\n",
       " Value(data=1.2902091746378512, grad=0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They each have a way to go. We'd like to push all 4 down to reach their respective targets. We do this by implementing a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=1.064391757661844, grad=0),\n",
       " Value(data=3.073141888428331, grad=0),\n",
       " Value(data=1.0, grad=0),\n",
       " Value(data=0.08422136504398284, grad=0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean squared error\n",
    "[(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They've been nudged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19 (main, May  6 2024, 19:43:03) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f347913e2b6fb5a69d1963ee938378d9a2045c59bfe2881ab986562c299cd819"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
