{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've learned about the multi-head attention mechanism. We will now learn to code the other building blocks of an LLM and assemble them into a GPT-like model.\n",
    "\n",
    "We start with a simplified version of a GPT-like model as an example. It consists of token and positional embeddings, dropout, a series of transformer blocks, a final layer of normalisation, and a linear output layer. The configuration is passed in via a python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # num of words used by BPE tokenizer\n",
    "    \"context_length\": 1024, # max num of input tokens model can handle\n",
    "    \"emb_dim\": 768,         # embedding size: each token -> 768-d vector\n",
    "    \"n_heads\": 12,          # num of heads in multi-head attention mechanism\n",
    "    \"n_layers\": 12,         # num of transformer blocks in the model\n",
    "    \"drop_rate\": 0.1,       # 10% random dropout of hidden units \n",
    "    \"qkv_bias\": False       # whether to include a bias vector in linear layers\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) # placeholder for transformer block\n",
    "              for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"]) # placeholder for layernorm\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module): # a placeholder to be replaced later\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x # block does nothing but return its input\n",
    "    \n",
    "\n",
    "class DummyLayerNorm(nn.Module): # a placeholder to be replaced later\n",
    "    def __init__(self, normalised_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method describes the data flow through the model: it computes token and positional embeddings for the input indices, applies dropout, processes the data through the transformer blocks, applies normalisation, and finally produces logits with the linear output layer.\n",
    "\n",
    "Now, we prepare the input data and initialise a new GPT model to illustrate its usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokeniser.encode(txt1)))\n",
    "batch.append(torch.tensor(tokeniser.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialise a new 124-mllion parameter DummyGPTModel instance and feed it the tokenised batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tensor has two rows, corresponding to the two text samples. Each text consists of 4 tokens; each token is a 50,257-dimensional vector, which matches the size of the tokeniser's vocab. Each of these dimensions refers to a unique token in the vocab. Later, we will convert these vectors back into token IDs, which we can then decode into words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising activations with layer normalisation\n",
    "\n",
    "Training deep neural networks with many layers can prove challenging due to problems like vanishing or exploding gradients. These lead to unstable training dynamics and make it difficult for the network to effectively adjust its weights - so the learning process struggles to find a set of parameters (weights) for the network that minimises the loss function. In other words, the network has difficulty learning the underlying patterns in the data, affecting its ability to make accurate predictions.\n",
    "\n",
    "<i>Layer normalisation</i> is designed to improve the stability and efficiency of neural network training. The idea is to adjust the activations (outputs) of a neural network layer to have a mean of 0 and a variance of 1 (a.k.a. unit variance). This speeds up the convergence to effective weights and ensures consistent, reliable training. In GPT-2, this is typically applied before and after the multi-head attention module and before the final output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.4091, 0.6587, 0.3914, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1902, 0.3182, 0.6486, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.rand(2, 5) # 2 training examples with 5 dimensions each (features)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.2432],\n",
      "        [0.1928]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0799],\n",
      "        [0.0670]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the means are above 0 and variances are far from 1. \n",
    "\n",
    "Using keepdim=True ensures that the ouput tensor retains the same number of dimensions as the input tensor, even though the operation (in this case mean and variance) reduces the tensor along the dimension specified via 'dim'. Without it, the returned mean tensor would be a two-dimensional vector insted of a 2x1-dimensional matrix. The dim parameter specifies the dimension along which the calculation should be performed. \n",
    "\n",
    "Dim=-1 (and dim=1 in this case as it's a 2-d tensor, so same thing) calculates the mean across the column dimension to obtain one mean per row (dim=0 would do so across the row dimension to obtain one mean per column). Later, when adding layer normalisation to the GPT model, which produces three-dimensional tensors with shape [batch_size, num_tokens, embedding_size], we still use dim=-1 for normalisation across the last dimension. \n",
    "\n",
    "Next, we apply layer normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised layer outputs:\n",
      " tensor([[-0.8603, -0.8603,  0.5869,  1.4698,  0.5242, -0.8603],\n",
      "        [-0.7450, -0.7450, -0.0102,  0.4844,  1.7608, -0.7450]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[0.0000e+00],\n",
      "        [9.9341e-09]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.],\n",
      "        [1.]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalised layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second value in the mean tensor above is 9e-09 - a value very close to zero. Small numerical errors can accumulate because of the finite precision with which computers represent numbers. We can improve readability by switching off scientific notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.],\n",
      "        [1.]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the layer norm class\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of layer normalisation operates on the last dimension of the input tensor x, which represents the embedding dimension. The variable 'eps' is a small constant added to the variance to prevent division by zero during normalisation. The 'scale' and 'shift' are two trainable parameters (of the same dimension as the input) that the LLM automatically adjusts during training. \n",
    "\n",
    "Note in the GPT class, we set 'unbiased = False'. In the variance calculation, we divide by the number of inputs n in the variance formula. Bessel's correction uses n-1 in the denominator to adjust for bias in the sample variance estimation. So dividing by n results in a biased estimate of the variance. For LLM's, the embedding dimension n is significantly large and so the difference between using n and n-1 is negligible. We are choosing this approach because it was used to implement the original GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.7999],\n",
      "        [0.7999]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "# For some reason, unbiased = False moved variance from 0.99 -> 0.80. Not in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular type of normalisation is batch normalisation, which normalises across the batch dimension, whereas layer normalisation normalises across the feature (embedding) dimension. LLMs require significant computational resources, and the specific hardware or use case can dictate the batch size during training or inference. Since layer normalisation normalises each input independently of the batch size, it offers more flexibility and stability in these scenarios. It is particularly beneficial for distributed training or when deploying models in environments where resources are constrained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a feed forward network with GELU activations\n",
    "\n",
    "Historically, the ReLU activation function has been used in deep learning due to its simplicity and effectiveness across various neural network architectures. In LLMs, the activation functions of choice tend to be the GELU (Gaussian error linear unit) and SwiGLU (Swish-gated linear unit). They are more complex and smooth activation functions incorporating Gaussian and sigmoid-gated linear units respectively, and offer improved performance. \n",
    "\n",
    "The GELU has an exact version, but the GPT-2 version implemented a computationally cheaper approximation. PyTorch can implement this approximate version but perhaps not at the time Raschka was writing this book, so what follows is the manual implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comparison of GELU and ReLU side by side\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100) # 100 sample data points between -3 and 3\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label} (x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smoothness of GELU can lead to better optimisation properties during training, as it allows for more nuanced adjustments to the model's parameters. In contrast, ReLU has a sharp corner at 0, which can sometimes make optimisation harder, especially in networks that are very deep or have complex architectures. ReLU ouputs 0 for any negative input, whereas GELU allows for a small, non-zero output for negative values, which means that during the training process, neurons that receive negative input can still contribute to the learning process, albeit to a lesser extent than positive inputs. \n",
    "\n",
    "Next, we will use GELU to implement a MLP class (Raschka calls it FeedForward). The FeedForward module plays a crucial role in the model's ability to learn from and generalise the data. Although the input and output dimensions of this module are the same, it internally expands the embedding dimension into a higher dimensional space through the first linear layer (shown in code 2 cells below), followed by the GELU activation function, and then a contraction back into the original dimension via the second linear layer. This design allows for the exploration of a richer representation space. The uniformity in input and output dimension simplifies the architecture by enabling the stacking of multiple layers, without the need to adjust dimensions between them, thus making the model more scalable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FeedForward module is a small neural network consisting of two linear layers and a GELU activation function. In the 124 million parameter GPT model, it receives the input batches with tokens that have an embedding size of 768 each, via the GPT_CONFIG_124M dictionary. \n",
    "\n",
    "So for example with a batch size of 32, 50_000 tokens, and an embedding size of 768, the journey looks like:\n",
    "1. Input tensor with shape (32, 50_000, 768).\n",
    "2. The first linear layer increases the embedding dimension by 4x -> (32, 50_000, 3072).\n",
    "3. This passes through the GELU activation function.\n",
    "4. The second linear layer decreases the embedding dimension by 4x -> (32, 50_000, 768) is the output tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# An example with our small batch example\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding shortcut connections\n",
    "\n",
    "Also known as skip or residual connections. They are there to mitigate the challenge of vanishing gradients (where gradients, which guide weight updates during training) become progressively smaller as they propagate backward through the layers, making it difficult to effectively train earlier layers. A shortcut connection creates an alternative, shorter path for the gradient to flow through the network, skipping one or more layers, which is achieved by adding the output of one layer to the output of a later layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural network to illustrate shortcut connections\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]),\n",
    "                          GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]),\n",
    "                          GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]),\n",
    "                          GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]),\n",
    "                          GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]),\n",
    "                          GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x) # compute output of current layer\n",
    "            if self.use_shortcut and x.shape == layer_output.shape: # check if shortcut applied\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep neural network with 5 layers, each consisting of a linear layer and a GELU activation function. In the forward pass, we iteratively pass the input through the layers and optionally add the shortcut connections if self.use_shortcut = True.\n",
    "\n",
    "Examples below. PyTorch computes the loss gradient for each layer in the model, so we can iterate through the weight parameters via model.named_parameters(). If we have a 3x3 weight parameter matrix for a given layer, the layer will have 3x3 gradient values. The function below prints the mean absolute gradient of these values to obtain a single gradient value per layer - to compare the gradients between layers more easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to compute the gradients in the model's backward pass\n",
    "def print_gradients(model, x):\n",
    "    output = model(x) # forward pass\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target) # calculate loss\n",
    "\n",
    "    loss.backward() # backward pass to calculate gradients\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the gradients become smaller as we progress from the last layer to the first layer, which is the <i>vanishing gradient problem</i>. As can be seen below, using shortcut connections means the gradient doesn't shrink to a vanishingly small value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732502937317\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "# Model with skip connections\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting attention and linear layers in a transformer block\n",
    "\n",
    "The transformer block is a fundamental building block of GPT and other LLM architectures. It is repeated a dozen times in the GPT-2 architecture and combines several concepts already covered: multi-head attention, layer normalisation, dropout, feed forward layers, and GELU activations. \n",
    "\n",
    "When a transformer block processes an input sequence, each element in the sequence (e.g. a word or sub-word token) is represented by a fixed-sized vector (in this case 768 dimensions). The operations within the transformer block are designed to transform these vectors in a way that preserves their dimensionality. The idea is that the self-attention mechanism in the multi-head attention block identifies and analyses relationships between elements in the input sequence. The feedforward network modifies the data individually at each position. This combination not only enables a more nuanced understanding and processing of the input but also enhances the model's overall capacity for handling complex data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer block in code\n",
    "from Chapter04 import MultiHeadAttention\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        shortcut = x \n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x) # shortcut connection for attention block\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # adds the original input back\n",
    "\n",
    "        shortcut = x \n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x) # shortcut connection for mlp block\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # adds the original input back\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformer block contains a multi-head attention block and a feed forward network, configured based on the configuration library. Layer normalisation is applied before each of these two components and dropout is applied after them to regularise the model and prevent overfitting (<i>Pre-LayerNorm</i>). The class also implements the forward pass, where each component is followed by a shortcut connection that adds the input of the block to its output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768) # batch_size, num_tokens, emb_dim\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer block maintains the dimensions from input to output. This is a crucial aspect of its design, which enables its effective application across a wide range of sequence-to-sequence tasks, where each output vector directly corresponds to an input vector, maintaining a one-to-one relationship. However, the output is a context vector that encapsulates information from the entire input sequence. This means that while the physical dimensions of the sequence (length and feature size) remain unchanged as it passes through the transformer block, the content of each output vector is re-encoded to integrate contextual information across the entire input sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the GPT Model\n",
    "\n",
    "We replace the DummyTransformerBlock and DummyLayerNorm placeholders with the updated versions. Thanks to the TransformerBlock class, the GPTModel class is relatively small and compact - but in GPT-2 the transformer block is repeated 12 times (n_layers in the dictionary) and 48 times in the largest GPT-2 model with 1,542 million parameters. The output from the final transformer block then goes through a final layer normalisation step before reaching the linear output layer. This layer maps the transformer's output to a high-dimensional space (in this case 50,257 dimensions corresponding to vocab size) to predict the next token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) \n",
    "              for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"]) \n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The init constructor initialises the token and positional embedding layers using the configurations passed in via the GPT dictionary. These embedding layers are responsible for converting input tokens indices into dense vectors and adding positional information. The init method also creates a sequential stack of transformer blocks, a layernorm layer is applied, standardising the outputs from the transformer blocks to stabilise the learning process. Finally, a linear output head without bias is defined, which project's the transformer's output into the vocab space of the tokeniser to generate logits for each token in the vocab.\n",
    "\n",
    "The forward method takes a batch of input token indices, computes their embeddings, applies the positional embeddings, passes the sequence through the transformer blocks, normalises the final output, and then computes the logits, representing the next token's unnormalised probabilities (these still need to be converted into tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1381,  0.0079, -0.1957,  ..., -0.0222, -0.1062,  0.1717],\n",
      "         [ 0.3867, -0.8400, -0.6558,  ..., -0.5162,  0.2362, -0.3349],\n",
      "         [ 0.6985, -0.1826, -0.1634,  ...,  0.1472, -0.6503, -0.0054],\n",
      "         [-0.4288,  0.1670, -0.1262,  ...,  1.1571,  0.5297, -0.5542]],\n",
      "\n",
      "        [[ 0.1095, -0.2890, -0.1463,  ..., -0.0557,  0.2907, -0.2818],\n",
      "         [ 0.0884, -0.3545, -0.3524,  ...,  1.2921,  0.0050,  0.1902],\n",
      "         [ 0.6092,  0.4702, -0.4093,  ...,  0.7682,  0.3781, -0.1968],\n",
      "         [-0.0608, -0.0739,  0.4747,  ...,  1.2458, -0.3834,  0.0612]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tensor has shape [2, 4, 50_257] since we passed in two input texts with four tokens each. The last dimension corresponds to the vocab size of the tokeniser. \n",
    "\n",
    "Using the numel() method (number of elements), we can collect the total number of parameters in the model's parameter tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there 163 million parameters instead of 124 million? Because of a concept called <i>weight tying</i>, used in the original GPT-2 architecture. It means that that model reuses the weights from the token embedding layer in its output layer. Look at the shapes of both layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight typing: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "# The above have the same shape\n",
    "# Let's remove the output layer parameter count from the model's parameter count\n",
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "    for p in model.out_head.parameters())\n",
    ")\n",
    "\n",
    "print(f\"Number of trainable parameters considering weight typing: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 124 million parameters. Weight tying reduces overall memory footprint and computational complexity of the model. However, using separate token embedding and output layers results in better training and model performance; hence in our implementation we use separate layers. \n",
    "\n",
    "Lastly, let's compute memory requirements. Assuming each parameter is a 32-bit float taking up 4 bytes, the total size of the model is 621.83MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
