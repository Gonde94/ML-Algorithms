{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Exercise 4.1</u></b>: Number of parameters in feed forward and attention modules\n",
    "\n",
    "Calculate and compare the number of parameters that are contained in the feed forward module and those in the multi-head attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix path to be able to import classes\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src folder to the Python path\n",
    "src_path = Path(\"../src\").resolve()  # Adjust the relative path based on where your notebook is\n",
    "sys.path.append(str(src_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "tokeniser = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokeniser.encode(txt1)))\n",
    "batch.append(torch.tensor(tokeniser.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chapter04 import GPTModel, GPT_CONFIG_124M\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in Transformer block: 85,026,816\n",
      "Total number of parameters in block per layer: 7,085,568\n"
     ]
    }
   ],
   "source": [
    "total_tblock = sum(p.numel() for p in model.trf_blocks.parameters())\n",
    "print(f\"Total number of parameters in Transformer block: {total_tblock:,}\")\n",
    "\n",
    "per_layer = round(total_tblock / 12)\n",
    "print(f\"Total number of parameters in block per layer: {per_layer:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in feed forward module: 4,722,432\n",
      "Total number of parameters in attention module: 2,360,064\n"
     ]
    }
   ],
   "source": [
    "# Easier to do it separately\n",
    "from Chapter04 import TransformerBlock\n",
    "\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "\n",
    "ff_params = sum(p.numel() for p in block.ff.parameters())\n",
    "print(f\"Total number of parameters in feed forward module: {ff_params:,}\")\n",
    "\n",
    "mha_params = sum(p.numel() for p in block.att.parameters())\n",
    "print(f\"Total number of parameters in attention module: {mha_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed forward module contains approx. twice as many parameters as the attention module. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
