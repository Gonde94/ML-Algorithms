{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix path to be able to import classes\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src folder to the Python path\n",
    "src_path = Path(\"../src\").resolve()  # Adjust the relative path based on where your notebook is\n",
    "sys.path.append(str(src_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Exercise 3.1</u></b>: Comparing SelfAttention_v1 and SelfAttention_v2.\n",
    "\n",
    "nn.Linear in SelfAttention_v2 uses a different weight initialisation scheme as nn.Parameter(torch.rand(d_in, d_out)) used in SelfAttention_v1, which causes the mechanisms to produce different results. To check that both implementations are otherwise similar, we can transfer the weight matrices from object v2 to v1, such that both objects then produce the same results. \n",
    "\n",
    "Correctly assign the weights from an instance of SelfAttention_v2 to SelfAttention_v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3751, 0.8610],\n",
      "        [1.4201, 0.8892],\n",
      "        [1.4198, 0.8890],\n",
      "        [1.3533, 0.8476],\n",
      "        [1.3746, 0.8606],\n",
      "        [1.3620, 0.8532]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Chapter03 import SelfAttention_v1, SelfAttention_v2, inputs, d_in, d_out\n",
    "\n",
    "torch.manual_seed(42)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3755, 0.2777],\n",
      "        [0.3761, 0.2831],\n",
      "        [0.3761, 0.2833],\n",
      "        [0.3768, 0.2763],\n",
      "        [0.3754, 0.2836],\n",
      "        [0.3772, 0.2746]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1 class weight: Parameter containing:\n",
      "tensor([[0.2566, 0.7936],\n",
      "        [0.9408, 0.1332],\n",
      "        [0.9346, 0.5936]], requires_grad=True)\n",
      "\n",
      "V2 class weight: Parameter containing:\n",
      "tensor([[-0.2811,  0.3391,  0.5090],\n",
      "        [-0.4236,  0.5018,  0.1081]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"V1 class weight:\", sa_v1.W_key)\n",
    "print(\"\\nV2 class weight:\", sa_v2.W_key.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2 class weight transposed:\n",
      " tensor([[-0.2811, -0.4236],\n",
      "        [ 0.3391,  0.5018],\n",
      "        [ 0.5090,  0.1081]], grad_fn=<PermuteBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"V2 class weight transposed:\\n\", sa_v2.W_key.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\n",
    "sa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\n",
    "sa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3755, 0.2777],\n",
      "        [0.3761, 0.2831],\n",
      "        [0.3761, 0.2833],\n",
      "        [0.3768, 0.2763],\n",
      "        [0.3754, 0.2836],\n",
      "        [0.3772, 0.2746]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensors are equal: True\n"
     ]
    }
   ],
   "source": [
    "equal = torch.equal(sa_v1(inputs), sa_v2(inputs))\n",
    "print(\"Tensors are equal:\", equal) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Exercise 3.2</u></b>: Returning two-dimensional embedding vectors\n",
    "\n",
    "Change the input arguments for the MultiHeadAttentionWrapper(..., num_heads=2) call such that the output context vectors are two-dimensional instead of four-dimensional, while keeping num_heads=2. You don't have to modify the class implementation, just change one of the other input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# Before\n",
    "from Chapter03 import MultiHeadAttentionWrapper\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0189, 0.2729],\n",
      "         [0.2181, 0.3037],\n",
      "         [0.2804, 0.3125],\n",
      "         [0.2830, 0.2793],\n",
      "         [0.2476, 0.2541],\n",
      "         [0.2748, 0.2513]],\n",
      "\n",
      "        [[0.0189, 0.2729],\n",
      "         [0.2181, 0.3037],\n",
      "         [0.2804, 0.3125],\n",
      "         [0.2830, 0.2793],\n",
      "         [0.2476, 0.2541],\n",
      "         [0.2748, 0.2513]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Just change d_out to 1\n",
    "d_in, d_out = 3, 1\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Exercise 3.3</u></b>: Initialising GPT-2 size attention modules\n",
    "\n",
    "Using the MultiHeadAttention class, initialise a multi-head attention module that has the same number of heads as the smallest GPT-2 model (12 attention heads). Ensure you use the respective input and output embedding sizes (768 dimensions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Chapter03 import MultiHeadAttention\n",
    "\n",
    "torch.manual_seed(42)\n",
    "context_length, d_in = 1_024, 768\n",
    "d_out = 768\n",
    "num_heads = 12\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads)\n",
    "mha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
