{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Exercise 2.1</u></b>: Byte pair encoding of unknown words\n",
    "\n",
    "Try the BPE tokenizer from tiktoken on the unknown words \"Akwirw ier\" and print the individual token IDs. Then, call the decode function on each of the resulting integers in this list to reproduce the mapping. Lastly, call the decode method on the token IDs to check whether it can reconstruct the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Akwirw ier\"\n",
    "integers = tokenizer.encode(phrase, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers) \n",
    "# [33_901, 86, 343, 86, 220, 959]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33901 : Ak\n",
      "86 : w\n",
      "343 : ir\n",
      "86 : w\n",
      "220 :  \n",
      "959 : ier\n"
     ]
    }
   ],
   "source": [
    "for i in integers:\n",
    "    mapping = tokenizer.decode([i])\n",
    "    print(i, \":\", mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Akwirw ier'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(integers) # done perfectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Exercise 2.2</u></b>: Dataloaders with different strides and context lengths\n",
    "\n",
    "To develop more of an intuition for how the dataloader works, try to run it with different settings such as max_length = 2 and stride = 2, and max_length = 8 and stride = 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in text\n",
    "with open(\"../the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nrad9\\Documents\\ML-Algorithms\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"}) # tokenizes the entire text\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride): # sliding window to chunk text into overlapping sequences of max_length\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self): # total number of rows in the dataset\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx): # returns a single row from the dataset\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # creates the dataset\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last, # if True, drops the last batch if it is smaller than specified batch_size\n",
    "        num_workers=num_workers # number of CPU processes to use for preprocessing\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch: [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "Second batch: [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "# The way done in class\n",
    "# max_length of 4 means 4 tokens per batch\n",
    "# stride of 1 means shift position along by 1 when grabbing second batch\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "second_batch = next(data_iter)\n",
    "print(\"First batch:\", first_batch)\n",
    "print(\"Second batch:\", second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch: [tensor([[ 40, 367]]), tensor([[ 367, 2885]])]\n",
      "Second batch: [tensor([[2885, 1464]]), tensor([[1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# max_length of 2 means 2 tokens per batch\n",
    "# stride of 2 means shift position along by 2 when grabbing second batch - no overlapping\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=2, stride=2, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "second_batch = next(data_iter)\n",
    "print(\"First batch:\", first_batch)\n",
    "print(\"Second batch:\", second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch: [tensor([[  40,  367, 2885, 1464, 1807, 3619,  402,  271]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899]])]\n",
      "Second batch: [tensor([[ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]\n"
     ]
    }
   ],
   "source": [
    "# max_length of 8 means 8 tokens per batch\n",
    "# stride of 2 means shift position along by 2 when grabbing second batch - back to overlapping\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=8, stride=2, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "second_batch = next(data_iter)\n",
    "print(\"First batch:\", first_batch)\n",
    "print(\"Second batch:\", second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch: [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "Second batch: [tensor([[1807, 3619,  402,  271]]), tensor([[ 3619,   402,   271, 10899]])]\n"
     ]
    }
   ],
   "source": [
    "# max_length of 4 means 4 tokens per batch\n",
    "# stride of 4 means shift position along by 4 when grabbing second batch - no overlapping\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=4, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "second_batch = next(data_iter)\n",
    "print(\"First batch:\", first_batch)\n",
    "print(\"Second batch:\", second_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
