{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "Attention is an integral part of the LLM architecture. To get a good understanding of it, we will implement 4 variants of attention mechanisms which build on each other, with the goal of arriving at a compact and efficient implementation of multi-head attention to plug into the LLM architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A simple self-attention mechanism without trainable weights\n",
    "\n",
    "This will highlight a few key concepts in self-attention before adding trainable weights.\n",
    "\n",
    "An input sequence is denoted as x, consisting of n elements represented as x<sup>1</sup> to x<sup>n</sup>. This sequence represents text that has already been transformed into token embeddings. E.g. below our input text is \"Your journey starts with one step\". Each element of the sequence, such as x<sup>1</sup>, corresponds to a d-dimensional embedding vector representing a single token, lke \"Your\" (in the example below, d = 3).\n",
    "\n",
    "In self-attention, the goal is to calculate context vectors z<sup>i</sup> for each element x<sup>i</sup> in the input sequence. A <i>context vector</i> can be interpreted as an enriched embedding vector. Each context vector z<suo>i</sup> contains information about x<sup>i</sup> and all other input elements, x<sup>1</sup> to x<sup>n</sup>. This is essential in an LLM, which needs to understand the relationship and relevance of words in a sentence to each other. In practice, trainable weights help an LLM learn these context vectors to help it generate the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input sequence\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your      (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey   (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts    (x^3)\n",
    "     [0.22, 0.58, 0.33], # with      (x^4)\n",
    "     [0.77, 0.25, 0.10], # one       (x^5)\n",
    "     [0.05, 0.80, 0.55]  # step      (x^6)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of implementing self-attention is to compute the intermediate values w, known as attention scores. We do this by computing the dot product between the query (as an example, x<sup>2</sup>) and every other input token. <u>A dot product is the multiplication of two vectors element-wise and then summing the products</u>. It is a measure of similarity because it quantifies how closely two vectors are aligned: a higher dot product indicates a higher degree of similarity between the vectors. For self-attention, the dot product determines the extent to which each element in a sequence focuses on (attends to) any other element. The higher the dot product, the higher the similarity and attention score between two elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # x^2\n",
    "atten_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    atten_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(atten_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{rounded_element:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 1: 0.43 * query position 1: 0.55 = 0.240\n",
      "Element 2: 0.15 * query position 2: 0.87 = 0.130\n",
      "Element 3: 0.89 * query position 3: 0.66 = 0.590\n",
      "Sum of dot products: 0.9544\n",
      "Using torch.dot(): tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "# Dot product manually\n",
    "res = 0\n",
    "for idx, element in enumerate(inputs[0]): # for each element in the first vector\n",
    "    print(f\"Element {idx+1}: {torch.round(element, decimals=2):.2f} * query position {idx+1}: \"\n",
    "          f\"{torch.round(query[idx], decimals=2):.2f} = \"\n",
    "          f\"{torch.round(element * query[idx], decimals=2):.3f}\") \n",
    "    res += inputs[0][idx] * query[idx]\n",
    "print(f\"Sum of dot products: {torch.round(res, decimals=4).item():.4f}\")\n",
    "print(\"Using torch.dot():\", torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then normalise the scores to obtain attention weights that sum up to 1. This is useful for interpretation and maintaining training stability in an LLM. In practice we use the softmax function, which is better at managing extreme values and offers more favourable gradient properties during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "atten_scores_2_tmp = atten_scores_2 / atten_scores_2.sum()\n",
    "print(\"Attention weights:\", atten_scores_2_tmp)\n",
    "print(\"Sum:\", atten_scores_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# With softmax\n",
    "atten_weights_2 = torch.softmax(atten_scores_2, dim=0)\n",
    "print(\"Attention weights:\", atten_weights_2)\n",
    "print(\"Sum:\", atten_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have normalised attention weights, we calculate the context vector z<sup>2</sup> by multiplying the embedded inputs tokens x<sup>i</sup>, with the corresponding attention weights and then summing the resulting vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# For x^2 alone\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += atten_weights_2[i] * x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there only 3 elements in the vector, rather than 6? Because each of the input tokens is 3-dimensional and the context vector is a weighted sum. Notice below, multiplying each 3-d input vector by the corresponding attention weight, also results in a 3-d vector. The context vector is the sum along those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input token vector: tensor([0.4300, 0.1500, 0.8900])\n",
      "First attention weight: tensor(0.1385)\n",
      "\n",
      "First row when calculating context vector: tensor([0.0596, 0.0208, 0.1233])\n",
      "Second row when calculating context vector: tensor([0.1308, 0.2070, 0.1570])\n",
      "Third row when calculating context vector: tensor([0.1330, 0.1983, 0.1493])\n",
      "Fourth row when calculating context vector: tensor([0.0273, 0.0719, 0.0409])\n",
      "Fifth row when calculating context vector: tensor([0.0833, 0.0270, 0.0108])\n",
      "Sixth row when calculating context vector: tensor([0.0079, 0.1265, 0.0870])\n"
     ]
    }
   ],
   "source": [
    "# One example\n",
    "print(\"First input token vector:\", inputs[0])\n",
    "print(\"First attention weight:\", atten_weights_2[0])\n",
    "print('')\n",
    "print(\"First row when calculating context vector:\", atten_weights_2[0] * inputs[0])\n",
    "print(\"Second row when calculating context vector:\", atten_weights_2[1] * inputs[1])\n",
    "print(\"Third row when calculating context vector:\", atten_weights_2[2] * inputs[2])\n",
    "print(\"Fourth row when calculating context vector:\", atten_weights_2[3] * inputs[3])\n",
    "print(\"Fifth row when calculating context vector:\", atten_weights_2[4] * inputs[4])\n",
    "print(\"Sixth row when calculating context vector:\", atten_weights_2[5] * inputs[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# For all input tokens\n",
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, each element in the tensor represents an attention score between each pair of inputs. The above can be done more concisely without the double for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# Normalise so each row sums to 1\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, by settng dim = -1, we instruct the softmax function to normalise along the last dimension of the tensor. Since it is a 2d-tensor [rows, columns], it will normalise across the columns so that the values in the rows sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Context vectors via matrix multiplication\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementing self-attention with trainable weights\n",
    "\n",
    "This is the mechanism used in the original Transformer architecture - also called <i>scaled dot-product attention</i>. The principle is the same as the previous section; the most notable difference is that we introduce weight matrices that are updated during model training. These are crucial so that the model (the attention module inside the model) can learn to produce \"good\" context vectors.\n",
    "\n",
    "The trainable weight matrices are W<sub>q</sub>, W<sub>k</sub>, and W<sub>v</sub>, which project the embedded input tokens x<sup>i</sup> into query, key, and value vectors respectively.\n",
    "\n",
    "A <i>query</i> is analogous to a search query in a database. It represents the current token the model focuses on or tries to understand. It is used to probe the other parts of the input sequence to determine how much attention to pay to them.\n",
    "\n",
    "A <i>key</i> is like a database key used for indexing. Each item in the input sequence (each token) has an associated key, which are used to match the query.\n",
    "\n",
    "A <i>value</i> is similar to the value in a key-value pair in a database. It represents the actual content of the input items. Once the model determines which keys (which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again we calculate one context vector\n",
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # input embedding size, d = 3\n",
    "d_out = 2 # output embeddng size, d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, in GPT-like models, the input and output dimensions are usually the same, but they're different here to better follow the calculation. Next, we initialise the three weight matrices. Requires_grad is set to False to reduce clutter in the outputs, but if we were to use the weight matrices for model training, we would set it to True to update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n"
     ]
    }
   ],
   "source": [
    "# What one looks like - random values of the right shape\n",
    "print(W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# Compute the vectors by multiping the input by the weight matrices\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2) # query vector for input vector 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 1: 0.55 * W_query row 1: tensor([0.2961, 0.5166])\n",
      "Result of matrix multiplication for row 1: tensor([0.1629, 0.2841])\n",
      " \n",
      "Element 2: 0.87 * W_query row 2: tensor([0.2517, 0.6886])\n",
      "Result of matrix multiplication for row 2: tensor([0.2190, 0.5990])\n",
      " \n",
      "Element 3: 0.66 * W_query row 3: tensor([0.0740, 0.8665])\n",
      "Result of matrix multiplication for row 3: tensor([0.0488, 0.5719])\n",
      " \n",
      "Sum along the columns: tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "# Manually\n",
    "# x_2 is one token * 3 dimensions\n",
    "# W_query is 3 rows by 2 dimensions (output dimension)\n",
    "row_mul = 0\n",
    "total = 0\n",
    "for idx, element in enumerate(x_2):\n",
    "    print(f\"Element {idx + 1}: {torch.round(element, decimals=2):.2f} * \"\n",
    "          f\"W_query row {idx + 1}: {W_query[idx]}\")\n",
    "    row_mul += x_2[idx] * W_query[idx]\n",
    "    total += x_2[idx] * W_query[idx]\n",
    "    print(f\"Result of matrix multiplication for row {idx + 1}: {row_mul}\")\n",
    "    row_mul = 0\n",
    "    print(' ')\n",
    "\n",
    "print(f\"Sum along the columns: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Note</u>: in the weight matrices, W, \"weight\" is short for \"weight parameters\", the values of a neural network that are optimised during training. This is not to be confused with attention weights, which determne the extent to which a context vector depends on the different parts of the input. So, weight parameters are the fundamental, learned coefficients, while attention weights are dynamic, context-specific values.\n",
    "\n",
    "We still require the key and value vectors for all the input elements as they are involved in computing the weights with respect to the query, q<sup>2</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys.shape: torch.Size([6, 2])\n",
      "Values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# For all the input vectors\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"Keys.shape:\", keys.shape)\n",
    "print(\"Values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six input tokens projected from a 3-d embedding space to a 2-d one. Next, compute the attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# Attention score w_22 only\n",
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(key_2) # query vector . key vector to compute attention score\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# To compute all attention scores for query 2\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2) # Note second element matches the value above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To move from attention scores to attention weights, we scale the attention scores using the softmax function, but by diving them using the square root of the embedding dimension of the keys (same as exponentiating by 0.5). This is done to improve training performance by avoiding small gradients. GPT-like LLMs have greater than 1000 dimensions, so large dot products can result in very small gradients when the softmax function is applied. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing 0, which slows down learning or causes training to stagnate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we compute the context vectors. We do so by computing it as a weighted sum over the value vectors. Attention weights serve as a weightng factor that weighs the respective importance of each value vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a compact self-attention class\n",
    "\n",
    "It is helpful to organise all the previous code into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is a subclass of nn.Module, which provides necessary functionalities for model layer creation and management. \n",
    "\n",
    "The <i>init</i> method initialises trainable weight matrices, each transforming the input dimension d_in to an output dimension d_out.\n",
    "\n",
    "The forward method computes the attention scores by multipling queries and keys, normalising these scores. Finally, it creates a context vector by weighting the values with these normalised attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since inputs contains 6 embedding vectors, this results in a matrix storing the six context vectors, with 2 columns matching d_out. The second row matches what we got manually before.\n",
    "\n",
    "We can improve the class further by utilising PyTorch's nn.Linear layers, which effectively perform matrix multiplication when the bias units are disabled. It also has an optimised weight initialisation scheme, contributing to more stable and effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Different outputs due to different initial weights\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
