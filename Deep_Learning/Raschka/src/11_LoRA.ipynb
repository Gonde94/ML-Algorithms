{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter-efficient fine-tuning with LoRA\n",
    "\n",
    "<i>Low-rank adaptation (LoRA)</i> is one of the most widely used techniques for parameter-efficient fine-tuning. LoRA is a technique that adapts a pretrained model to better suit a specific, often smaller dataset by adjusting only a small subset of the model's weight parameters. The \"low-rank\" aspect refers to the mathematical concept of limiting model adjustments to a smaller dimensional subspace of the total weight parameter space. This effectively captures the most influential directions of the weight parameter changes during training. LoRA is useful and popular because it enables efficient fine-tuning of large models on task-specific dat, significantly cutting down on computational costs and resources usually required for fine-tuning. \n",
    "\n",
    "Suppose a large weight matrix W is associated with a specific layer (LoRA can be applied to all linear layers in an LLM but we focus on a single layer for illustration purposes). During backpropagation, we learn a ΔW matrix, which contains information on how much we want to update the original weight parameters to minimise the loss function during training (from now on \"weight\" = model's weight parameters). In regular training and fine-tuning, the weight update is defined as<br>\n",
    "\n",
    "W<sub>updated</sub> = W + WΔ\n",
    "\n",
    "The LoRA method offers a more efficient alternative to computing the weight updates by learning an approximation of it:\n",
    "\n",
    "ΔW ≈ AB\n",
    "\n",
    "where A and B are two matrices much smaller than W, and AB represents the matrix multiplication product between A and B. Using LoRA, we can reformulate the weight update defined earlier:\n",
    "\n",
    "W<sub>updated</sub> = W + AB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_parquet(\"../Datasets/train.parquet\")\n",
    "valid_df = pd.read_parquet(\"../Datasets/valid.parquet\")\n",
    "test_df = pd.read_parquet(\"../Datasets/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from Chapter05 import tokeniser\n",
    "from Chapter06 import SpamDataset\n",
    "\n",
    "train_dataset = SpamDataset(\"../Datasets/train.parquet\", \n",
    "                            max_length=None,\n",
    "                            tokeniser=tokeniser\n",
    ")\n",
    "val_dataset = SpamDataset(\"../Datasets/valid.parquet\", \n",
    "                            max_length=None,\n",
    "                            tokeniser=tokeniser\n",
    ")\n",
    "test_dataset = SpamDataset(\"../Datasets/test.parquet\", \n",
    "                            max_length=None,\n",
    "                            tokeniser=tokeniser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 109])\n",
      "Target batch dimensions: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Target batch dimensions:\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialising the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
