{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter-efficient fine-tuning with LoRA\n",
    "\n",
    "<i>Low-rank adaptation (LoRA)</i> is one of the most widely used techniques for parameter-efficient fine-tuning. LoRA is a technique that adapts a pretrained model to better suit a specific, often smaller dataset by adjusting only a small subset of the model's weight parameters. The \"low-rank\" aspect refers to the mathematical concept of limiting model adjustments to a smaller dimensional subspace of the total weight parameter space. This effectively captures the most influential directions of the weight parameter changes during training. LoRA is useful and popular because it enables efficient fine-tuning of large models on task-specific data, significantly cutting down on computational costs and resources usually required for fine-tuning. \n",
    "\n",
    "Suppose a large weight matrix W is associated with a specific layer (LoRA can be applied to all linear layers in an LLM but we focus on a single layer for illustration purposes). During backpropagation, we learn a ΔW matrix, which contains information on how much we want to update the original weight parameters to minimise the loss function during training (from now on \"weight\" = model's weight parameters). In regular training and fine-tuning, the weight update is defined as<br>\n",
    "\n",
    "W<sub>updated</sub> = W + WΔ\n",
    "\n",
    "The LoRA method offers a more efficient alternative to computing the weight updates by learning an approximation of it:\n",
    "\n",
    "ΔW ≈ AB\n",
    "\n",
    "where A and B are two matrices much smaller than W, and AB represents the matrix multiplication product between A and B. Using LoRA, we can reformulate the weight update defined earlier:\n",
    "\n",
    "W<sub>updated</sub> = W + AB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_parquet(\"../Datasets/train.parquet\")\n",
    "valid_df = pd.read_parquet(\"../Datasets/valid.parquet\")\n",
    "test_df = pd.read_parquet(\"../Datasets/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from Chapter05 import tokeniser\n",
    "from Chapter06 import SpamDataset\n",
    "\n",
    "train_dataset = SpamDataset(\"../Datasets/train.parquet\", \n",
    "                            max_length=None,\n",
    "                            tokeniser=tokeniser\n",
    ")\n",
    "val_dataset = SpamDataset(\"../Datasets/valid.parquet\", \n",
    "                            max_length=None,\n",
    "                            tokeniser=tokeniser\n",
    ")\n",
    "test_dataset = SpamDataset(\"../Datasets/test.parquet\", \n",
    "                            max_length=None,\n",
    "                            tokeniser=tokeniser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 109])\n",
      "Target batch dimensions: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Target batch dimensions:\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialising the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from Chapter05 import model_configs, load_weights_into_gpt\n",
    "from Chapter04 import GPTModel\n",
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50_257,\n",
    "    \"context_length\": 1_024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you to the point of the \"I-don-it-from-the\n"
     ]
    }
   ],
   "source": [
    "# Double-check coherent text is generated. Clearly not.\n",
    "from Chapter04 import generate_text_simple\n",
    "from Chapter05 import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokeniser),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokeniser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace output layer for classification fine-tuning\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification accuracy of not-fine-tuned model\n",
    "from Chapter06 import calc_accuracy_loader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "        train_loader, model, device, num_batches=10\n",
    ")\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 53.75%\n",
      "Test accuracy: 58.75%\n"
     ]
    }
   ],
   "source": [
    "# No better than random guessing\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter-efficient fine-tuning with LoRA\n",
    "\n",
    "We initialise a LoRA-Layer that creates the matrices A and B, along with the 'alpha' scaling factor and the 'rank' (r) setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank governs the inner dimension of matrices A and B. Essentially this determines the number of extra parameters introduced by LoRA, which creates balance between the adaptability of the model and its efficiency via the number of parameters used.\n",
    "\n",
    "Alpha is a scaling factor for the output from the low-rank adaptation. It primarily dictates the degree to which the output from the adapted layer can affect the original layer's output. This can be seen as a way to regulate the effect of the low-rank adaptation on the layer's output. \n",
    "\n",
    "In LoRA, the typical goal is to substitute existing Linear layers, allowing weight updates to be applied directly to the pre-existing pretrained weights. To integrate the original Linear layers, we create a LinearWithLoRA layer, which utilises the previously implemented LoRALayer and is designed to replace the existing Linear layers within a neural network, such as the self-attention modules or feed-forward modules in GPTModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above combines a standard Linear layer with the LoRA layer. The forward method computes the output by adding the results from the original linear layer and the LoRA layer. \n",
    "\n",
    "Since the weight matrix B (self.B in LoRALayer) is initialised with zero values, the product of matrices A and B will be a zero matrix. This ensures that the multiplication does not alter the original weights, as adding zero does not change them. \n",
    "\n",
    "To apply LoRA, we introduce a replace_linear_lora_function, which will swap all existing Linear layers in GPTModel with the newly created LinearWithLoRA layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters before: 124,441,346\n",
      "Total number of trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "# Freeze original model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduced the number of trainable parameters by 50x when using LoRA. A rank and alpha of 16 are good default choices, but it is also common to increase the rank parameter, which in turn increases the number of trainable parameters. Alpha is usually chosen to be half, double, or equal to the rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Verify layers have been modified as intended\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 53.75%\n",
      "Test accuracy: 58.75%\n"
     ]
    }
   ],
   "source": [
    "# Calculate initial classification accuracy before fine-tuning\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "        train_loader, model, device, num_batches=10\n",
    ")\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are identical to the previous values - we initialised the LoRA matrix with zeros. Now we will fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 000000): Train loss 0.809, Val loss 0.909\n",
      "Epoch 1 (Step 000050): Train loss 0.078, Val loss 0.257\n",
      "Epoch 1 (Step 000100): Train loss 0.454, Val loss 0.300\n",
      "Training accuracy: 95.00% | Validation accuracy: 90.00%\n",
      "Epoch 2 (Step 000150): Train loss 0.400, Val loss 0.387\n",
      "Epoch 2 (Step 000200): Train loss 0.528, Val loss 0.526\n",
      "Epoch 2 (Step 000250): Train loss 0.022, Val loss 0.253\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Epoch 3 (Step 000300): Train loss 0.451, Val loss 0.313\n",
      "Epoch 3 (Step 000350): Train loss 0.357, Val loss 0.537\n",
      "Training accuracy: 100.00% | Validation accuracy: 82.50%\n",
      "Epoch 4 (Step 000400): Train loss 0.126, Val loss 0.296\n",
      "Epoch 4 (Step 000450): Train loss 0.009, Val loss 0.303\n",
      "Epoch 4 (Step 000500): Train loss 0.108, Val loss 0.240\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Epoch 5 (Step 000550): Train loss 0.012, Val loss 0.149\n",
      "Epoch 5 (Step 000600): Train loss 0.057, Val loss 0.190\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Training completed in 45.15 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from Chapter06 import train_classifier_simple\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
    "    train_classifier_simple(\n",
    "        model, train_loader, val_loader, optimiser, device,\n",
    "        num_epochs=num_epochs, eval_freq=50, eval_iter=5\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_mins = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_mins:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAABxx0lEQVR4nO3dd1hT5xcH8G8S9kbZskRRHAwFpYhbFPeq1Vp/ddRqXVWLWmutu3W0aq3WVVu1rbPujSJu3CAKgjgZKkNU9k7e3x+viURBGYEkeD7Pk6fk5ubekyvl5L7jvALGGAMhhBBCVJJQ2QEQQgghpHSUqAkhhBAVRomaEEIIUWGUqAkhhBAVRomaEEIIUWGUqAkhhBAVRomaEEIIUWGUqAkhhBAVRomaEEIIUWGUqAmpQWJjYyEQCBAeHq7sUAghCkKJmhAVIxAI3vmYO3euskMkhFQjDWUHQAiRl5iYKPt5586dmD17NmJiYmTbDAwMlBEWIURJ6I6aEBVjZWUlexgbG0MgEMieW1hYYPny5bC1tYW2tjY8PDwQGBhY6rHEYjG++OILuLi4ID4+HgBw4MABNG/eHDo6OnBycsK8efNQVFQke49AIMCff/6Jfv36QU9PD87Ozjh48KDs9ZcvX2LIkCEwNzeHrq4unJ2dsWnTplJj2L17N1xdXaGrq4vatWvDz88P2dnZstf//PNPNGrUCDo6OnBxccGaNWvk3p+QkICBAwfCxMQEtWrVQp8+fRAbGyt7ffjw4ejbty+WLl0Ka2tr1K5dG+PHj0dhYWGZrzkhKo0RQlTWpk2bmLGxsez58uXLmZGREdu+fTu7c+cO+/bbb5mmpia7e/cuY4yxR48eMQDsxo0bLC8vj/Xr1481a9aMpaSkMMYYO3fuHDMyMmKbN29mDx48YCdOnGCOjo5s7ty5snMAYLa2tmzbtm3s3r17bOLEiczAwIA9f/6cMcbY+PHjmYeHB7t27Rp79OgRCwoKYgcPHiwx/qdPnzINDQ22fPly9ujRI3br1i22evVqlpmZyRhjbMuWLcza2prt2bOHPXz4kO3Zs4fVqlWLbd68mTHGWEFBAWvUqBH74osv2K1bt1hUVBT77LPPWMOGDVl+fj5jjLFhw4YxIyMjNmbMGBYdHc0OHTrE9PT02B9//KHYfwxClIQSNSEq7M1EbWNjw3766Se5fVq0aMHGjRvHGHudqM+fP886derEWrduzdLS0mT7durUiS1cuFDu/f/++y+ztraWPQfAfvjhB9nzrKwsBoAdO3aMMcZYr1692IgRI8oUf2hoKAPAYmNjS3y9Xr16bNu2bXLbFixYwHx8fGSxNWzYkEkkEtnr+fn5TFdXlx0/fpwxxhO1g4MDKyoqku3zySefsEGDBpUpRkJUHfVRE6ImMjIy8PTpU/j6+spt9/X1xc2bN+W2DR48GLa2tjh16hR0dXVl22/evImQkBD89NNPsm1isRh5eXnIycmBnp4eAMDNzU32ur6+PoyMjJCSkgIAGDt2LD7++GOEhYWhS5cu6Nu3L1q1alVizO7u7ujUqRNcXV3h7++PLl26YMCAATA1NUV2djYePHiAkSNHYtSoUbL3FBUVwdjYWBbv/fv3YWhoKHfcvLw8PHjwQPa8SZMmEIlEsufW1taIiIh4x9UkRH1QoiakBurevTu2bNmCS5cuoWPHjrLtWVlZmDdvHvr37//We3R0dGQ/a2pqyr0mEAggkUgAAN26dUNcXByOHj2KoKAgdOrUCePHj8fSpUvfOqZIJEJQUBAuXryIEydOYNWqVZg5cyauXLki+1KwYcMGeHt7v/U+abyenp7YunXrW8c2NzcvU7yEqDtK1ISoCSMjI9jY2CAkJATt2rWTbQ8JCUHLli3l9h07diyaNm2K3r1748iRI7L9mzdvjpiYGNSvX79SsZibm2PYsGEYNmwY2rRpg2nTppWYqAGeNH19feHr64vZs2fDwcEB+/btQ0BAAGxsbPDw4UMMGTKkxPc2b94cO3fuhIWFBYyMjCoVMyHqihI1IWpk2rRpmDNnDurVqwcPDw9s2rQJ4eHhJd5xfv311xCLxejZsyeOHTuG1q1bY/bs2ejZsyfs7e0xYMAACIVC3Lx5E5GRkfjxxx/LFMPs2bPh6emJJk2aID8/H4cPH0ajRo1K3PfKlSsIDg5Gly5dYGFhgStXruDZs2ey/efNm4eJEyfC2NgYXbt2RX5+Pq5fv46XL18iICAAQ4YMwS+//II+ffpg/vz5sLW1RVxcHPbu3Ytvv/0Wtra2Fb+YhKgJStSEqJGJEyciPT0dU6ZMQUpKCho3boyDBw/C2dm5xP0nT54MiUSC7t27IzAwEP7+/jh8+DDmz5+PJUuWQFNTEy4uLvjyyy/LHIOWlhZmzJiB2NhY6Orqok2bNtixY0eJ+xoZGeHcuXNYsWIFMjIy4ODggGXLlqFbt24AgC+//BJ6enr45ZdfMG3aNOjr68PV1RWTJ08GAOjp6eHcuXOYPn06+vfvj8zMTNSpUwedOnWiO2zywRAwxpiygyCEEEJIyajgCSGEEKLCKFETQgghKowSNSGEEKLCKFETQgghKowSNSGEEKLCKFETQgghKowSdSWsXr0ajo6O0NHRgbe3N65evarskKrMuXPn0KtXL9jY2EAgEGD//v1yrzPGMHv2bFhbW0NXVxd+fn64d++e3D4vXrzAkCFDYGRkBBMTE4wcORJZWVly+9y6dQtt2rSBjo4O7Ozs8PPPP1f1R1OIRYsWoUWLFjA0NISFhQX69u0rt4Y0wOtTjx8/HrVr14aBgQE+/vhjJCcny+0THx+PHj16QE9PDxYWFpg2bZrcEpQAcObMGTRv3hza2tqoX78+Nm/eXNUfTyHWrl0LNzc3GBkZwcjICD4+Pjh27Jjs9Q/9+pRk8eLFEAgEsnnlAF0nAJg7dy4EAoHcw8XFRfZ6jbtGSl4URG3t2LGDaWlpsY0bN7Lbt2+zUaNGMRMTE5acnKzs0KrE0aNH2cyZM9nevXsZALZv3z651xcvXsyMjY3Z/v372c2bN1nv3r1Z3bp1WW5urmyfrl27Mnd3d3b58mV2/vx5Vr9+fTZ48GDZ6+np6czS0pINGTKERUZGsu3btzNdXV22fv366vqYFebv7882bdrEIiMjWXh4OOvevTuzt7dnWVlZsn3GjBnD7OzsWHBwMLt+/Tr76KOPWKtWrWSvFxUVsaZNmzI/Pz9248YNdvToUWZmZsZmzJgh2+fhw4dMT0+PBQQEsKioKLZq1SomEolYYGBgtX7eijh48CA7cuQIu3v3LouJiWHff/8909TUZJGRkYwxuj5vunr1KnN0dGRubm5s0qRJsu10nRibM2cOa9KkCUtMTJQ9nj17Jnu9pl0jStQV1LJlSzZ+/HjZc7FYzGxsbNiiRYuUGFX1eDNRSyQSZmVlxX755RfZtrS0NKatrc22b9/OGGMsKiqKAWDXrl2T7XPs2DEmEAjYkydPGGOMrVmzhpmamsrWGWaMsenTp7OGDRtW8SdSvJSUFAaAnT17ljHGr4empibbtWuXbJ/o6GgGgF26dIkxxr8MCYVClpSUJNtn7dq1zMjISHZNvv32W9akSRO5cw0aNIj5+/tX9UeqEqampuzPP/+k6/OGzMxM5uzszIKCgli7du1kiZquEzdnzhzm7u5e4ms18RpR03cFFBQUIDQ0FH5+frJtQqEQfn5+uHTpkhIjU45Hjx4hKSlJ7noYGxvD29tbdj0uXboEExMTeHl5yfbx8/ODUCjElStXZPu0bdsWWlpasn38/f0RExODly9fVtOnUYz09HQAQK1atQAAoaGhKCwslLtGLi4usLe3l7tGrq6usLS0lO3j7++PjIwM3L59W7ZP8WNI91G33zuxWIwdO3YgOzsbPj4+dH3eMH78ePTo0eOtz0LX6bV79+7BxsYGTk5OGDJkCOLj4wHUzGtEiboCUlNTIRaL5f6RAcDS0hJJSUlKikp5pJ/5XdcjKSkJFhYWcq9raGigVq1acvuUdIzi51AHEokEkydPhq+vL5o2bQqAx6+lpQUTExO5fd+8Ru/7/KXtk5GRgdzc3Kr4OAoVEREBAwMDaGtrY8yYMdi3bx8aN25M16eYHTt2ICwsDIsWLXrrNbpOnLe3NzZv3ozAwECsXbsWjx49Qps2bZCZmVkjrxEtykGIgo0fPx6RkZG4cOGCskNROQ0bNkR4eDjS09Oxe/duDBs2DGfPnlV2WCojISEBkyZNQlBQkNz64ESedFEXAHBzc4O3tzccHBzw33//QVdXV4mRVQ26o64AMzMziESit0YRJicnw8rKSklRKY/0M7/relhZWSElJUXu9aKiIrx48UJun5KOUfwcqm7ChAk4fPgwTp8+LbcEo5WVFQoKCpCWlia3/5vX6H2fv7R9jIyM1OIPlJaWFurXrw9PT08sWrQI7u7u+O233+j6vBIaGoqUlBQ0b94cGhoa0NDQwNmzZ7Fy5UpoaGjA0tKSrlMJTExM0KBBA9y/f79G/i5Roq4ALS0teHp6Ijg4WLZNIpEgODgYPj4+SoxMOerWrQsrKyu565GRkYErV67IroePjw/S0tIQGhoq2+fUqVOQSCTw9vaW7XPu3DkUFhbK9gkKCkLDhg1hampaTZ+mYhhjmDBhAvbt24dTp06hbt26cq97enpCU1NT7hrFxMQgPj5e7hpFRETIfaEJCgqCkZERGjduLNun+DGk+6jr751EIkF+fj5dn1c6deqEiIgIhIeHyx5eXl4YMmSI7Ge6Tm/LysrCgwcPYG1tXTN/l6p9+FoNsWPHDqatrc02b97MoqKi2OjRo5mJiYncKMKaJDMzk924cYPduHGDAWDLly9nN27cYHFxcYwxPj3LxMSEHThwgN26dYv16dOnxOlZzZo1Y1euXGEXLlxgzs7OctOz0tLSmKWlJfv8889ZZGQk27FjB9PT01OL6Vljx45lxsbG7MyZM3JTRnJycmT7jBkzhtnb27NTp06x69evMx8fH+bj4yN7XTplpEuXLiw8PJwFBgYyc3PzEqeMTJs2jUVHR7PVq1erzbSa7777jp09e5Y9evSI3bp1i3333XdMIBCwEydOMMbo+pSm+Khvxug6McbYlClT2JkzZ9ijR49YSEgI8/PzY2ZmZiwlJYUxVvOuESXqSli1ahWzt7dnWlparGXLluzy5cvKDqnKnD59mgF46zFs2DDGGJ+iNWvWLGZpacm0tbVZp06dWExMjNwxnj9/zgYPHswMDAyYkZERGzFiBMvMzJTb5+bNm6x169ZMW1ub1alThy1evLi6PmKllHRtALBNmzbJ9snNzWXjxo1jpqamTE9Pj/Xr148lJibKHSc2NpZ169aN6erqMjMzMzZlyhRWWFgot8/p06eZh4cH09LSYk5OTnLnUGVffPEFc3BwYFpaWszc3Jx16tRJlqQZo+tTmjcTNV0nPk3K2tqaaWlpsTp16rBBgwax+/fvy16vaddIwBhj1X8fTwghhJCyoD5qQgghRIVRoiaEEEJUGCVqQgghRIVRoiaEEEJUGCVqQgghRIVRoiaEEEJUGCXqSsjPz8fcuXORn5+v7FBUGl2n96Nr9H50jd6PrtH7qeM1onnUlZCRkQFjY2Okp6fDyMhI2eGoLLpO70fX6P3oGr0fXaP3U8drRHfUhBBCiAqjRE0IIYSosA9uPeqioiLcuHEDlpaWEAor9z0lMzMTAPDkyRNkZGQoIrwaia7T+9E1ej+6Ru9H1+j9VOUaSSQSJCcno1mzZtDQeHcq/uD6qK9du4aWLVsqOwxCCCEEV69eRYsWLd65zwd3R21paQmAXxxra2slR0MIIeRDlJiYiJYtW8py0rt8cIla2txtbW0NW1tbJUdDCCHkQ1aWLlgaTEYIIYSoMErUhBBCiAqjRE0IIYSosA+uj5oQQspDLBajsLBQ2WEQNaOpqQmRSKSQY1GiJoSQEjDGkJSUhLS0NGWHQtSUiYkJrKysIBAIKnUcStSVkZ4OnDsH2NsD7u7KjoYQokDSJG1hYQE9Pb1K/7ElHw7GGHJycpCSkgIAlZ4KTIm6Mn74Afj9d2D8eP5fQkiNIBaLZUm6du3ayg6HqCFdXV0AQEpKCiwsLCrVDE6DySqjY0f+31OnlBsHIUShpH3Senp6So6EqDPp709lxzhQoq6Mdu0AgQCIjgaePlV2NIQQBaPmblIZivr9oURdGbVqAc2a8Z9Pn1ZuLIQQQmokStSVRc3fhJAaztHREStWrCjz/mfOnIFAIKjyEfObN2+GiYlJlZ5DFVCirixK1IQQFSEQCN75mDt3boWOe+3aNYwePbrM+7dq1QqJiYkwNjau0PmIPBr1XVlt2gAaGkBsLPDoEVC3rrIjIoR8oBITE2U/79y5E7Nnz0ZMTIxsm4GBgexnxhjEYvF710IGAHNz83LFoaWlBSsrq3K9h5SO7qgry8AA8PbmP9NdNSFEiaysrGQPY2NjCAQC2fM7d+7A0NAQx44dg6enJ7S1tXHhwgU8ePAAffr0gaWlJQwMDNCiRQucPHlS7rhvNn0LBAL8+eef6NevH/T09ODs7IyDBw/KXn+z6VvaRH38+HE0atQIBgYG6Nq1q9wXi6KiIkycOBEmJiaoXbs2pk+fjmHDhqFv377lugZr165FvXr1oKWlhYYNG+Lff/+VvcYYw9y5c2Fvbw9tbW3Y2Nhg4sSJstfXrFkDZ2dn6OjowNLSEgMGDCjXuasKJWpFoOZvQmo8xhhyCoqq/cEYU+jn+O6777B48WJER0fDzc0NWVlZ6N69O4KDg3Hjxg107doVvXr1Qnx8/DuPM2/ePAwcOBC3bt1C9+7dMWTIELx48aLU/XNycrB06VL8+++/OHfuHOLj4zF16lTZ60uWLMHWrVuxadMmhISEICMjA/v37y/XZ9u3bx8mTZqEKVOmIDIyEl999RVGjBiB068G++7Zswe//vor1q9fj3v37mH//v1wdXUFAFy/fh0TJ07E/PnzERMTg8DAQLRt27Zc568q1PStCB07AgsW8ETNGJ+yRQipUXILxWg8+3i1nzdqvj/0tBT3p3r+/Pno3Lmz7HmtWrXgXqyy4oIFC7Bv3z4cPHgQEyZMKPU4w4cPx+DBgwEACxcuxMqVK3H16lV07dq1xP0LCwuxbt061KtXDwAwYcIEzJ8/X/b6qlWrMGPGDPTr1w8A8Pvvv+Po0aPl+mxLly7F8OHDMW7cOABAQEAALl++jKVLl6JDhw6Ij4+HlZUV/Pz8oKmpCXt7e7Rs2RIAEB8fD319ffTs2ROGhoZwcHBAM+msHiWjO2pF+OgjQEcHSEric6oJIURFeXl5yT3PysrC1KlT0ahRI5iYmMDAwADR0dHvvaN2c3OT/ayvrw8jIyNZycyS6OnpyZI0wMtqSvdPT09HcnKyLGkCgEgkgqenZ7k+W3R0NHx9feW2+fr6IvrV3+VPPvkEubm5cHJywqhRo7Bv3z4UFRUBADp37gwHBwc4OTnh888/x9atW5GTk1Ou81cVuqNWBB0dwNcXCA7md9WNGys7IkKIgulqihA1318p51UkfX19uedTp05FUFAQli5divr160NXVxcDBgxAQUHBO4+jqakp91wgEEAikZRrf0U367+PnZ0dYmJicPLkSQQFBWHcuHH45ZdfcPbsWRgaGiIsLAxnzpzBiRMnMHv2bMydOxfXrl1T+hQwuqNWlN69gZ49AQcHZUdCCKkCAoEAeloa1f6o6upoISEhGD58OPr16wdXV1dYWVkhNja2Ss/5JmNjY1haWuLatWuybWKxGGFhYeU6TqNGjRASEiK3LSQkBI2L3Tzp6uqiV69eWLlyJc6cOYNLly4hIiICAKChoQE/Pz/8/PPPuHXrFmJjY3FKBcYe0R21okycyB+EEKJGnJ2dsXfvXvTq1QsCgQCzZs16551xVfn666+xaNEi1K9fHy4uLli1ahVevnxZri8q06ZNw8CBA9GsWTP4+fnh0KFD2Lt3r2wU++bNmyEWi+Ht7Q09PT1s2bIFurq6cHBwwOHDh/Hw4UO0bdsWpqamOHr0KCQSCRo2bFhVH7nMKFETQsgHbPny5fjiiy/QqlUrmJmZYfr06cjIyKj2OKZPn46kpCQMHToUIpEIo0ePhr+/f7lWnerbty9+++03LF26FJMmTULdunWxadMmtG/fHgBfH3rx4sUICAiAWCyGq6srDh06hNq1a8PExAR79+7F3LlzkZeXB2dnZ2zfvh1NmjSpok9cdgJW3Z0ESvb48WPY2dkhISEBtra2ij9BXByQmQk0bar4YxNCqkVeXh4ePXqEunXrQkdHR9nhfJAkEgkaNWqEgQMHYsGCBcoOp0Le9XtUnlyk9D7q1atXw9HRETo6OvD29sbVq1ffuf+KFSvQsGFD6Orqws7ODt988w3y8vKqKdr32LABcHQEpk1TdiSEEKJW4uLisGHDBty9excREREYO3YsHj16hM8++0zZoSmdUhP1zp07ERAQgDlz5iAsLAzu7u7w9/cvdYj/tm3b8N1332HOnDmIjo7GX3/9hZ07d+L777+v5si5E7eTMP9QFB6/fDWEv2VLQCQCior4fGpCCCFlIhQKsXnzZrRo0QK+vr6IiIjAyZMn0ahRI2WHpnRK7aNevnw5Ro0ahREjRgAA1q1bhyNHjmDjxo347rvv3tr/4sWL8PX1lX3DcnR0xODBg3HlypVqjVtq3dkHCItPQxMbI9h66gGursCLF4CRkVLiIYQQdWVnZ/fWiG3CKe2OuqCgAKGhofDz83sdjFAIPz8/XLp0qcT3tGrVCqGhobLm8YcPH+Lo0aPo3r17qefJz89HRkaG7JGZmamwz+DlWAsAcD3upfQDUJImhBCiUEq7o05NTYVYLIalpaXcdktLS9y5c6fE93z22WdITU1F69atwRhDUVERxowZ886m70WLFmHevHkKjV3K08EUABAaV0J929xcQFe3Ss5LCCHkw6H0wWTlcebMGSxcuBBr1qxBWFgY9u7diyNHjrxzROCMGTOQnp4ue0RFRSksHmmivpuchfScQr4xLY2vpmVmxpM1IYQQUglKu6M2MzODSCRCcnKy3Pbk5ORS1zGdNWsWPv/8c3z55ZcAAFdXV2RnZ2P06NGYOXMmhMK3v3doa2tDW1tb9lyR8wPNDLRR10wfj1KzERb/Eh1cLABjY+DpUyAnB7h4EejUSWHnI4QQ8uFR2h21lpYWPD09ERwcLNsmkUgQHBwMHx+fEt+Tk5PzVjKWToZX1nTw5vb8rvq6tPlbIKBlLwkhhCiMUpu+AwICsGHDBvz999+Ijo7G2LFjkZ2dLRsFPnToUMyYMUO2f69evbB27Vrs2LEDjx49QlBQEGbNmoVevXqVq3qNInk5vkrUsS9fb5Qm6mJfQgghhJCKUGqiHjRoEJYuXYrZs2fDw8MD4eHhCAwMlA0wi4+PR2Jiomz/H374AVOmTMEPP/yAxo0bY+TIkfD398f69euV9RHg9aqf+ubjNBSKX9XH7dCB//faNSA9XUmREUJIxbRv3x6TJ09+5z6Ojo5YsWJFpc+lqOO8y+bNm5W+AlZlKH0w2YQJExAXF4f8/HxcuXIF3t7estfOnDmDzZs3y55raGhgzpw5uH//PnJzcxEfH4/Vq1cr9R+gnrkBjHU1kVcowe2nr/q/7e2B+vUBiQQ4f15psRFCPiy9evVC165dS3zt/PnzEAgEuHXrVjVHxZWWLK9du4bRo0dXf0BqROmJWt0JhQLZ6O/rscWmaVE/NSGkmo0cORJBQUF4/PjxW69t2rQJXl5ecHNzU0JkpTM3N4eenp6yw1BplKgV4PV86hL6qSlRE0KqSc+ePWFubi7XEgkAWVlZ2LVrF0aOHInnz59j8ODBqFOnDvT09ODq6ort27e/87gpKSno1asXdHV1UbduXWzduvWtfZYvXw5XV1fo6+vDzs4O48aNQ1ZWFgDeOjpixAikp6dDIBBAIBBg7ty5AN5u+o6Pj0efPn1gYGAAIyMjDBw4UG520Ny5c+Hh4YF///0Xjo6OMDY2xqefflruYlZr165FvXr1oKWlhYYNG+Lff/+VvcYYw9y5c2Fvbw9tbW3Y2NhgYrFljNesWQNnZ2fo6OjA0tISAwYMKNe5y4sStQJI+6mvx718Pfpc2k998yaQmqqkyAghCpedXf5HUdHr9xcV8W1v1lko6X3lpKGhgaFDh2Lz5s1yM2F27doFsViMwYMHIy8vD56enjhy5AgiIyMxevRofP755+9cEGn48OFISEjA6dOnsXv3bqxZs+atNRmEQiFWrlyJ27dv4++//8apU6fw7bffAuBVJVesWAEjIyMkJiYiMTERU6dOfes8EokEffr0wYsXL3D27FkEBQXh4cOHGDRokNx+Dx48wP79+3H48GEcPnwYZ8+exeLFi8t8nfbt24dJkyZhypQpiIyMxFdffYURI0bg9OnTAIA9e/bg119/xfr163Hv3j3s378frq6uAIDr169j4sSJmD9/PmJiYhAYGIi2bduW+dwVwj4wCQkJDABLSEhQ2DFzC4pY/e+PMIfph1lcavbrF1xdGQMY++8/hZ2LEFL1cnNzWVRUFMvNzX37Rb7kTvkexf8G/Pcf39aunfxxzczefl8FREdHMwDs9OnTsm1t2rRh//vf/0p9T48ePdiUKVNkz9u1a8cmTZrEGGMsJiaGAWBXr1596xy//vprqcfctWsXq127tuz5pk2bmLGx8Vv7OTg4yI5z4sQJJhKJWHx8vOz127dvy51/zpw5TE9Pj2VkZMj2mTZtGvP29i41ljfP3apVKzZq1Ci5fT755BPWvXt3xhhjy5YtYw0aNGAFBQVvHWvPnj3MyMhI7vyledfvUXlyEd1RK4COpghNbIwBFJtPDVDzNyGk2rm4uKBVq1bYuHEjAOD+/fs4f/48Ro4cCQAQi8VYsGABXF1dUatWLRgYGOD48eOIj48v8XjR0dHQ0NCAp6en3DneHBh28uRJdOrUCXXq1IGhoSE+//xzPH/+HDk5OWWOPTo6GnZ2drCzs5Nta9y4MUxMTBAdHS3b5ujoCENDQ9lza2vrUlddLO08vr6+ctt8fX1l5/jkk0+Qm5sLJycnjBo1Cvv27UPRq1aRzp07w8HBAU5OTvj888+xdevWcn3GiqBErSDFm79lKFETUvNkZZX/0a/f6/f368e3HTsmf9zY2LffV0EjR47Enj17kJmZiU2bNqFevXpo164dAOCXX37Bb7/9hunTp+P06dMIDw+Hv78/CgoKKny+2NhY9OzZE25ubtizZw9CQ0OxevVqAKjUcUujqakp91wgEEAikSjs+HZ2doiJicGaNWugq6uLcePGoW3btigsLIShoSHCwsKwfft2WFtbY/bs2XB3d0daWprCzv8mStQKIi18Elq88EnbtnxFraQk4OXLUt5JCFEr+vrlf2gUq9asocG3vbloT0nvq6CBAwdCKBRi27Zt+Oeff/DFF19AIBAAAEJCQtCnTx/873//g7u7O5ycnHD37t1Sj+Xi4oKioiKEhobKtsXExMglptDQUEgkEixbtgwfffQRGjRogKdPn8odR0tLC2Kx+J1xN2rUCAkJCUhISJBti4qKQlpaGho3blyeS/De87y5pGZISIjcOXR1ddGrVy+sXLkSZ86cwaVLlxAREQGAjwXw8/PDzz//jFu3biE2NhanqvCGTKnrUdckng58ycu7KZlIzy2Esa4mYGIChIcDjRrJ/49KCCFVyMDAAIMGDcKMGTOQkZGB4cOHy15zdnbG7t27cfHiRZiammL58uVITk4uNRE2bNgQXbt2xVdffYW1a9dCQ0MDkydPhm6xLxr169dHYWEhVq1ahV69eiEkJATr1q2TO46joyOysrIQHBwMd3d36OnpvTUty8/PD66urhgyZAhWrFiBoqIijBs3Du3atYOXl5fCrs+0adMwcOBANGvWDH5+fjh06BD27t2LkydPAuBzvsViMby9vaGnp4ctW7ZAV1cXDg4OOHz4MB4+fIi2bdvC1NQUR48ehUQiQcOGDRUW35vojlpBzA214VBbD4wBYfHF7p5dXSlJE0Kq3ciRI/Hy5Uv4+/vDxsZGtv2HH35A8+bN4e/vj/bt28PKygp9+/Z957E2bdoEGxsbtGvXDv3798fo0aNhYWEhe93d3R3Lly/HkiVL0LRpU2zduhWLFi2SO0arVq0wZswYDBo0CObm5vj555/fOo9AIMCBAwdgamqKtm3bws/PD05OTti5c2flLsYb+vbti99++w1Lly5FkyZNsH79emzatAnt27cHAJiYmGDDhg3w9fWFm5sbTp48iUOHDqF27dowMTHB3r170bFjRzRq1Ajr1q3D9u3b0aRJE4XGWJyAMSWtZqEkjx8/hp2dHRISEmBra6vQYwf8F469YU8woUN9TPWvum9XhJCqlZeXh0ePHqFu3brQ0dFRdjhETb3r96g8uYjuqBXI61Xzt9zIb8aACRN4SdF795QUGSGEEHVFiVqBpAPKwhOKLdAhEAAREcCDB8CryfSEEEJIWVGiVqD65gYw0tFAXqEEUdIFOgBg5kzg8GHg00+VFxwhhBC1RIlageQW6Cg+n7pLF6BHD8DISEmREUIIUVeUqBXMy5H3U4cW76cmhBBCKogStYK9XvLypVxRfISFAd9/D+zapaTICCHlpchqV+TDo6jfH5rgq2DutibQEAqQkpmPxy9zYVfr1YT+oCBg0SKgd2/gk0+UGyQh5J20tLQgFArx9OlTmJubQ0tLS1bZi5D3YYyhoKAAz549g1AohJaWVqWOR4lawXS1RGhSxxg3E9JwPe7F60TdqRP/75kzfJk7KoJCiMoSCoWoW7cuEhMT3yqFSUhZ6enpwd7eHkJh5RqvKVtUAS8HU56oY1+iX7NXE9mbNQOMjYH0dODGDaBFC+UGSQh5Jy0tLdjb26OoqOi9NaoJeZNIJIKGhoZCWmIoUVcBLwdT/HXhEUKLj/wWiYD27YEDB/hqWpSoCVF5AoEAmpqab63WREh1osFkVcDzVeGTmGS+QIeMdNnL4GAlREUIIUQdUaKuAhaGOrCvxRfouBFfwvrUFy4A+fnKCY4QQohaoURdRbxeTdMKK9783aQJYGEB5OYCV64oKTJCCCHqhBJ1FZE2f8tVKBMIXt9VV+Ei44QQQmoOStRVRLqSVnhCGorExSa9U6ImhBBSDpSoq4izBV+gI6dAjOjEzNcvSBP15ctAdrZygiOEEKI2lJ6oV69eDUdHR+jo6MDb2xtXr1595/5paWkYP348rK2toa2tjQYNGuDo0aPVFG3ZCYUCNJct0FGs7reTE2BvDxQWAiEhSoqOEEKIulBqot65cycCAgIwZ84chIWFwd3dHf7+/khJSSlx/4KCAnTu3BmxsbHYvXs3YmJisGHDBtSpU6eaIy8br5JW0ireT03TtAghhLyHUgueLF++HKNGjcKIESMAAOvWrcORI0ewceNGfPfdd2/tv3HjRrx48QIXL16UFSBwdHSszpDLxfNVP3XoqwU6ZBVqhgwBGjTgS18SQggh76C0O+qCggKEhobCz8/vdTBCIfz8/HDp0qUS33Pw4EH4+Phg/PjxsLS0RNOmTbFw4UKVLe/nYccX6EjKyMOTtNzXL/j5ATNmAG5uyguOEEKIWlBaok5NTYVYLIalpaXcdktLSyQlJZX4nocPH2L37t0Qi8U4evQoZs2ahWXLluHHH38s9Tz5+fnIyMiQPTIzM0vdV9F0tURoYmMEAPLlRAkhhJAyUvpgsvKQSCSwsLDAH3/8AU9PTwwaNAgzZ87EunXrSn3PokWLYGxsLHs0bty4GiPG6wFlsW8k6ufPgR07gJ07qzUeQggh6kVpidrMzAwikQjJycly25OTk2FlZVXie6ytrdGgQQOIRCLZtkaNGiEpKQkFBQUlvmfGjBlIT0+XPaKiohT3IcpAOp/6+pt31KdPA4MHA+9oDSCEEEKUlqi1tLTg6emJ4GIjnyUSCYKDg+Hj41Pie3x9fXH//n1IJK8LiNy9exfW1talLsytra0NIyMj2cPQ0FCxH+Q9vKQLdCRlIDOv2AId7dsDHh6Avz9Q7PMQQgghxSm16TsgIAAbNmzA33//jejoaIwdOxbZ2dmyUeBDhw7FjBkzZPuPHTsWL168wKRJk3D37l0cOXIECxcuxPjx45X1Ed7L0kgHtqa6kDDgRnza6xfMzPi61EuXApVcVJwQQkjNpdTpWYMGDcKzZ88we/ZsJCUlwcPDA4GBgbIBZvHx8RAWS2J2dnY4fvw4vvnmG7i5uaFOnTqYNGkSpk+frqyPUCZeDqZ4/DIX1+Neom0Dc2WHQwghRI0IGGNM2UFUp8ePH8POzg4JCQmwtbWtlnP+ezkOs/ZHwrd+bWz98iP5F3NygOvXgbZtqyUWQgghyleeXKTUO+oPhbRC2Y14vkCHhuhVK0FeHl/2MjsbiI0FHByUFyQhhBCVRJ2j1aCBpSEMtfkCHXeSis3j1tEBXF35z6dPKyc4QgghKo0SdTUQCQVoJptP/UL+RVr2khBCyDtQoq4mJS7QAQCdOvH/njoFfFjDBQghhJQBJepqIk3Ub5US9fEBtLWBJ0+Ae/eUEBkhaqaoCHj4UNlREFJtKFFXEw97E4iEAiSmv7FAh64u0KoV/5mWvSTk3SQS4JNPgHr1gD//VHY0hFQLStTVRE9LA42t+QId1E+t/gIjk7Aq+B7EEuquqFbLlwP79/OfJ00C7t5VajiEVAdK1NXIs7Tmb2miPn2ayomquLxCMWbsjcCYLaFYFnQXQVHJ738TUYyLFwHpOvW2trwGwf/+BxQWvvt9hKg5StTVSFr3+62VtFq0AAwM+IpaERFKiIyURWxqNmZOW49D5+8AAHQK85C59Ff6clUdnj8HBg0CxGLg00950jYxAa5do4VtSI1XoUSdkJCAx48fy55fvXoVkydPxh9//KGwwGoi6Upad5IykJVf9PoFTc3Xlcmo+VslHYtIxD/DZ2DxqolYfWw5pneuh33/TsUnf/+MwrWlL7NKFEAiAYYOBR4/Bho0AP74A7CzA9au5a//9BNw9apyYySkClUoUX/22Wc4/apAR1JSEjp37oyrV69i5syZmD9/vkIDrEmsjHVQx0S6QEcpzd+UqFVKQZEEC/aFI2X4KMw+sgqaEjFaNrXDmFYOCPTugSeG5rgpMFZ2mDXb0qXA0aO8QNCuXYB0BbxPPwWGDAFGjQKaNFFujIRUoQol6sjISLRs2RIA8N9//6Fp06a4ePEitm7dis2bNysyvhqn1OZvaaI+e5ZPPyFK9yQtF18sP46O3wzDsLAjAADx/AXQ3bUTAl1d5Hz5FTp/uQb/mDZWcqQ1GGPAhQv855UrATc3+df//pvfWevrV39shFSTCiXqwsJCaGtrAwBOnjyJ3r17AwBcXFyQmJiouOhqoFLnU7u7A3XqAN7eQGqqEiIjxZ2+k4Kvf9iCHxeNhG/cLRTp6gH790M06wdAIAAAdGtmixwtXQRHJyOvUEwFa6qCQMBHeR84AHz55duvi0Svf5ZIgPv3qy00QqpLhRJ1kyZNsG7dOpw/fx5BQUHo2rUrAODp06eoXbu2QgOsaTxf9VPfiH+JInGxQUhCIRAXBwQFAVZWSoqOFIkl+DnwDv6ZtQZ/b5gEx7REFNk7QOPKZaBPH7l9m9mZoI6JLnLyC/Fg4a+8eE1enpIir2GKf+kRCoHevWVfkEqUmgp07sxrEiTTSHxSs1QoUS9ZsgTr169H+/btMXjwYLi7uwMADh48KGsSJyVraMUX6Mh+c4EOQP7ugFS7lIw8DNlwGUVLfsZfu+fDsCAXkjZtoHH92uvFU4oRCATo1tQK+gV5sF2xBLhyBViyRAmR10BLlgBffMGnYJWFoSFP1tnZwI0bVRsbIdWsQom6ffv2SE1NRWpqKjZu3CjbPnr0aKxbRyNg30UkFMDD3gRACc3fUsnJQEFB9QVFcPF+KvouC8bA33/A92c2QQgGjB4N4cmTgLl5qe/r7maNLG09zO80mm9YuJCKcFTWkyfAnDnApk3AoUNle4+2NrBjB0/Sr1r4CKkpKpSoc3NzkZ+fD1NT3t8aFxeHFStWICYmBhYWFgoNsCaSTtN6a4EOgDffWVkBISHVHNWHSSJhWBl8D5N/C8TqDVPw8e3TYCIRsGoVsG4doKX1zvc3szOBjbEO9tTzQapve/4Fa9w46q+ujDp1gMBAICAAGDiw7O9r1IhP3yKkhqlQou7Tpw/++ecfAEBaWhq8vb2xbNky9O3bF2ulcxtJqaQjv0PfLCUKAJaW/L+RkdUY0YfpeVY+hm++huVBd8EYULcgDczUFILjx4EJE97dJ/qKQCBAN1drQCDAmk++4VOIgoOB7dur4RPUYB06AMuWlenfoETnz/Oa4FS1jNQAFUrUYWFhaNOmDQBg9+7dsLS0RFxcHP755x+sXLlSoQHWRB52fIGOp+l5eFp8gQ6AF2949gz4+mvlBPeBuB77Aj1WXsC5u8+goynEt8PbweRkIARXrrxeerSMurtaAwD+e6GNwhnf843ffAO8LKVrg5Rs0yYgJqbyx8nKAvr1A3bvpqplpEaoUKLOycmB4auiAydOnED//v0hFArx0UcfIS4uTqEB1kT62hpoZM2v31vN3w4OgJmZEqL6MDDGsOHcQ3y6/iI+O7wBXyZcwv7xvvjEyw7w8ACcnct9zGZ2JrA21kFWfhHO9BoKuLgAKSnA998r/gPUVGfP8ulXXl7Ao0eVO5aBAfD77/znn34CLl+ufHyEKFGFEnX9+vWxf/9+JCQk4Pjx4+jSpQsAICUlBUZGRgoNsKaS9lOX2PytBnIKijDnQCR+OX4HEY/TwdSgTzY9pxCj/w3FT0ej0TviFCZe2omZe5fBpSijUscVCgXo1pTfVR+JefG6tOX69TU2SSh01bDkZGDwYD4P+uOPgbp1K3/MTz8FPvuM1wb/3//4XTYhaqpCiXr27NmYOnUqHB0d0bJlS/j4+ADgd9fNmjVTaIA1lXQlrRIHlB06BLRvD/zwQ/UGVQ5/X4zD35fisPr0A/T6/QJaLzmN+YeicPXRC5Vc+jHicTp6/n4eQVHJ0BIJ0XzGBLCPP4Zg/Xq+ElMl9XDjifpkdAryfNsAw4bxAWVjxtS4SnNXHj5Ho1mB+HjtxbdL4ZaXNJEmJgKNGwOrVysmSIAfy84OePCAD0wjRE1VKFEPGDAA8fHxuH79Oo4fPy7b3qlTJ/z6668KC64mkw4oi058Y4EOAMjM5E2Bx44pIbL3KyiSYPNF3jzZzN4EupoiPEnLxcaQRxi4/hK8F57EjL0ROHf3GQqKlLuyFGMM/16KxcdrL8L09k04GYmwZ2wr/M/XCYJdu/hiDwpQvPn7/L1U4JdfgFq1gJs3eenLGmT9uYcoEEsQGvcS/dZcxMTtN/D4ZRnnO79p4ULg5ElAT4/X8VZkKVATE15iVCAANmwADh5U3LEJqUYVXubSysoKzZo1w9OnT2UrabVs2RIuLi4KC64mszbWlS3QER6fJv9ihw78vzduAC9Ur2n8SMRTJGfkw9xQGztGf4Qbszvjj8890b95HRjpaCA1qwDbr8Zj6Mar8PoxCAE7w3H8dhJyC8TVGmdWfhEm7gjHrAO30T/0KPZunYbAqG1wrfOqe6aiI4pLULz5+2hEIp97/fPP/MXZs4GEBIWdS5kS03NxJiYFANDD1RoCAXDw5lN0XHYWSwLvIDOvHKOsT58G5s7lP69dy++oFa1DB2DKFP7zl19S1TKiliqUqCUSCebPnw9jY2M4ODjAwcEBJiYmWLBgASS0Nm+ZvW7+fiMZW1vzP1qM8TtrFcIHY/G76WE+DtDWEEFHU4QuTaywfKAHQmd1xr8jW+Izb3uYGWgjI68Ie288wVf/hqL5giCM3RKKA+FPkFGeP+gVcCcpA71XXcDRGwmYd3I9Fh//HRpiMbSKCqpsyk4PN176NSjqVe3vESMAX19eNSs2tkrOWd12XX8MCQNa1q2F1UOa4/DXreHjVBsFRRKsPfMA7X85gy2X4+TL45YkOZn3IUskvAKZglo2SvTjj3wxj2fPgJEjaY47UTsVStQzZ87E77//jsWLF+PGjRu4ceMGFi5ciFWrVmHWrFmKjrHGks2nLqmfWkWXvbz08DmiEjOgoynEEG+Ht17XFAnRxtkcC/u54sr3nbBrjA9Gtq6LOia6yC0U41hkEibtCIfngiAM33QVO67G43lWvkJj3HU9AX1Xh+B5QhK275uHYaGvqlv9+COf3/yeIiYV1czOFFZGxZq/hUJg61bgzh3g1XRGdSaRMOy8xlsGBre0AwA0sTHGtlHe+HOoF5zM9PE8uwA/7I9Et9/Oy+683yIW8+Upk5KApk15cZmqpK0NbNnC/92PHOHrWROiRgSsAsN1bWxssG7dOtmqWVIHDhzAuHHj8OTJE4UFqGiPHz+GnZ0dEhISYKuAQUSVcftpOnqsvAADbQ3cnNMFImGxpth9+4D+/fmd9e3bygvyDV9svoZTd1Lwv4/s8WPft+tfl4YxhttPMxAYmYTA20m4n/J6FK5QwO/QujaxQpcmVrAx0a1QbLkFYsw+EIldoY9RLzUBWw/9BKuUx7zfc8sWoG/fCh23POYduo1NIbHo16wOfh3kUeXnq07n7j7D0I1XYaSjgasz/aCjKV+bvlAswdbLcVgRfA9pObzVoo2zGWb2aAQXq2KzQebN403e+vrA9et8Olt1WL6cN4Pr6fFuJapiRpSoPLmoQnfUL168KLEv2sXFBS8q0Ke6evVqODo6QkdHB97e3rh69WqZ3rdjxw4IBAL0rYY/wFXBxcoIBtoayMovQsybC3S0a8f7UKOi+IhYFXA/JQun7qRAIAC+8C3fFBqBQICmdYwx1b8hTga0w8mAtpjm3xBN6xhBwoDLD19g7qEotFp8Cn1Wh2DtmQd4lJpd5uM/eJaFfmtCsCv0MTo+vIaj26fxJO3gAFy8WC1JGuD9tgBwMioZ+UXF+uQZA/79F1i0qFriqArSu+l+zeq8laQB3poy3Lcuzk7tgFFt6kJTJMD5e6no/tt5zNgbgWeZ+byFaN48/oZ166ovSQPA5Mm8pSonB/j8c6paRtRGhRK1u7s7fpcWFCjm999/h9ubC7u/x86dOxEQEIA5c+YgLCwM7u7u8Pf3R0pKKc1mr8TGxmLq1KmyCmnqSCQUoJlsgY43vuDUqgVIp7qdPl29gZXirwu8b7qTiyWczA0qdaz6FoYY36E+Dn/dBue/7YAfejRCC0dTCATAzYQ0LAm8gw5Lz8D/13NYHnQXUU8zSp2rfejmU/RedQF3EjMwOfwg/to9H9o5Wby5+do13j9ZTZrb8+bvzPwinL9bbF3xkBDeDztrlkq1kJTV86x8nIhKAgAMamH/zn2N9TQxs0djnAxoh+6uVpAwYPvVeAyYswc5AwbxLy1ffsmnZVUnoRDYvJmX6R04kFarI2pDoyJv+vnnn9GjRw+cPHlSNof60qVLSEhIwNGjR8t1rOXLl2PUqFEYMWIEAGDdunU4cuQINm7ciO+++67E94jFYgwZMgTz5s3D+fPnkZaWVpGPoRKa25vi/L1UXI97ic99HOVf7NgRCAvjdyGffaaU+KSeZ+Vjbxgf3T+qjQIKUhRjV0sPX7ZxwpdtnJCSmYegqGQERibh0oPniEnORExyJlYG34N9LT10bWqFrk2t4GFrgkKJBD8ejsa/l+OgXVSAv0M2oN3lV1PaRo3i1amqqD+6NEKhAN1crbApJBZHIxLh1/hV7fbWrXliatKkQtXPlG1v2BMUihncbI3R2KZsRY0cautjzRBPXIt9gR8PRyE6thA7nXzQ+slt3Bk5HT0kDEKh4kbel4mdHfDwIW/+JkRNVChRt2vXDnfv3sXq1atx584dAED//v0xevRo/Pjjj2W+yy0oKEBoaChmzJgh2yYUCuHn54dLly6V+r758+fDwsICI0eOxPnz5yvyEVSGdEDZ9dhSBpQtXaoSA8q2XI5HfpEErnWM0bJurSo7j4WhDoZ4O2CItwPScwoRfIcn7bN3nyH+RQ7+OPcQf5x7CEsjbRjqaOJ+ShbMs17iQPBS2Ny5ye+Sfv21zItqVIUertbYFBKLoFfN39oar+7c/vlHaTFVBmMMO67FAwA+fc/ddElaONbCvnG+OHTrKZbUNsTPz9KQuz8Gf15Pwg89G6OFY9X9PpWoeJLOyeF3+Iqcv02IglUoUQN8QNlPP/0kt+3mzZv466+/8EcZR1WmpqZCLBbDUrpi1CuWlpayLwBvunDhAv766y+Eh4eX6Rz5+fnIz389qjgzM/Mde1e/ZvamEAqAJ2m5SErPg5WxzusX27QBNDR47eNHjxRTWrEC8grF+PdyLADgyzZ1IaimZGOsp4n+zW3Rv7ktcgqKcCbmGQIjk3DqTgqSM/KRnJEPEz1NrOzUADbb4nmBi127AD+/aomvNNLm76SMPJy/m/r6rrr4dSsoAPLyADUouXs97iUePMuGrqYIvdyty3+AmBgI69VDH4868G9ihb8uPMKa0/dx83E6Pll3Cd2aWuG7bi5wqF3NyTI0lLdUtWtHI8GJSqtwwRNlyMzMxOeff44NGzbArIwLVyxatAjGxsayR+OqKKpQCQbaGrIRsW/NpzYwALy9+c9K7Kc+EP4EqVkFsDbWka0UVd30tDTQ3dUaKwc3Q+gsP2wa3gLf+DXAkYlt4NO5JXDgAHD1qtKTNMCbv7s25XOqj0aUMBDw2jWgeXN+168Gdlzlg8h6uVvDUEezfG9++pR/4ezUCXj2DDqaIozvUB9npnXA4Jb2EAqAY5FJ8Ft+Fj8diUJ6TjUO8MrIAO7dA44eVcnCQoRIKTVRm5mZQSQSIfmNakHJycmwsrJ6a/8HDx4gNjYWvXr1goaGBjQ0NPDPP//g4MGD0NDQwIMHD956z4wZM5Ceni57REVFVdnnqaj3Nn8DwJkz1RdQMYwx/HmeDyIb4esITZHyv9tpx8ehw6JpmFR4H3WkU7natFGpvl9p7e+gN0d/A7zIR1QUHwWuIgMFS5OeW4gjEU8BvH8QWYmiooDcXCA9nX/xfMXcUBuL+rvi2KS2aNvAHIVihg3nH6Hd0tPYHPIIhe8rmKIIHTrwaXu3bvHBm4SoKKX+1dXS0oKnpyeCg4Nl2yQSCYKDg2WD1IpzcXFBREQEwsPDZY/evXujQ4cOCA8Ph52d3Vvv0dbWhpGRkewhXZ5TlUgrlJVY+GTYMJ6kN2yo3qBeOXv3Ge6lZEFfS1SxP9RV4fBh3t87eDCvNqWCPO1NYWmkjcz8Ily4lyr/orc3X6wDAMaOBfIVW/BFkQ7efIq8QgmcLQzQ/NUMhXLx8+NNzLt3A7pvz49vaGWIf75oic0jWqCBpQHScgox91AU/H89h6Co5Kpfle2zzyhJE5VXrj7q/v37v/P1ioy+DggIwLBhw+Dl5YWWLVtixYoVyM7Olo0CHzp0KOrUqYNFixZBR0cHTZs2lXu/iYkJALy1XZ14vRpME5WYgez8IuhrF/tnqVePP5REOiVrUAt7GOuWs9lTUZKTgcePAU9P/vyrr3ihjIkTeU1tFSSt/b35YiyORCSiUyP5cRhYuBDYuxeIieELeKjoSmk7pYPIWtqXb2yCRMKnQwFlKizSvqEFWtc3w87rCVh+4i4epmZj1D/X4eNUGzN7NELTOsYVCb/sGONf/hgDhg+v2nMRUk7luqMu3tdb0sPBwQFDy1mzd9CgQVi6dClmz54NDw8PhIeHIzAwUDbALD4+HokqUvCjqtQx0YW1sQ7EEoabCWnKDkcmOjED5++lQijgzd7V7sULYMYMwMmJl5yULheprc3/qHp5VX9M5fDO5m8TEz46HeClTe/fr97gyiDySToin2RASyREv2Z1yv7GJ08AD49yN+triHhZ2jPT2mNc+3rQ0hDi0sPn6PX7BUzbdRPJGXnl+wDlcegQT9DjxwN371bdeQipgAqVEFVnqlRCtLgJ28Jw+FYiAjo3wMROb/S1xsTwtXU1NYFly6otpqm7bmJ36GP0cLXG6iHNq+28yMgAfvuNT03LyODbWrTgZVXrlCNhKJlEwuCzOBjJGfnYONwLHV3euKtmDOjShS/z2KULEBioUtO3ftgfgS2X49HL3QarBpdxnfmiIj6u4vx5/kXqypXXd9bl9PhlDn4OjMHBm7yPXFdThK/aOWF0WyfoaVV4wkrJJBKgc2c+FbJlS+DCBf7/GyFVpMpLiBLF85KtpFVCP3VaGl+4YNMm/gelGqRk5OFAOK/ZPlLBBU5KlZPDk7OTE18aMiODVxU7cID/wVejJA3IL3155FbS2zsIBMCaNbyF4MQJ4L//qjnC0uUUFOHADZ4gP23x9tiPUs2ezZO0oSFfAKWCSRoAbE31sHJwM+wb1wqeDqbILRRjxcl76LD0DHaHPlZs/7W0apmJCZ898OOPijs2IZVEiVpFSPupb8S9hFjyxh8gT0/eH7thA195qBr8cykOhWIGTwdTNLc3rdqT5efzFoP69YFp04Dnz3m/5o4dfPGE3r1V6k6zPKTT2U5EJb3d/A3wkerff89/njyZfylTAUcjkpCZXwT7WnrwcapdtjcdO/a6lvlff/F/TwVoZm+K3WN8sPqz5rCrpYvkjHxM3XUTa868PcujUuzs+LrYAPDTT8Dly4o9PiEVRIlaRbhYGUJPS4TM/CLcTX6jKIuGBm8K/vjjammOyykowpYrcQCAL1tX4d10URGwcSNPyhMm8MVHHBz4ttu3gUGDKnVHpgq8HExhYaiNzLwihNxPLXmn6dP5NUhKUplBZdJBZINa2JWtzGdCAl/oAuD9vJ98otB4BAIBerhZ42RAO1nX0PKguyXPlKiMTz/lI8HFYl7yNSvr/e8hpIqp91/BGkRDJJQt0FFi83c12hP2BGk5hbCrpYsuTd6ez64Q167xJTxHjgTi4wFra35XffcuMGIE/3JSA/Dmb34NS2z+BnjT95o1/Oc1a/i1UaL7KZm4FvsSIqEAAzzLMI6jsJAnuOfPeSGXKhxHoa0hwjd+zujtbgOxhGHi9htIz1VwkZTVq/nd9YMHQECAYo9NSAVQolYhng68+Ts0toQqSRIJX4Hpxx+rdHk+iYRh46spWV/41pVfI1uR7Oz4lKvatXm/9P37wLhx1b6IRnWQNn8HRSWhoKiUMQadOvGR7Yzx6WfSEe5KIF3OskNDC1ga6bxnb/BWgIsXeTnU//7jXzyqkEAgwE/9msK+lh6epOXi+70Riu2vNjEB/v6bd7ds2AAcPKi4YxNSAZSoVcg7B5QBfE3lWbOq9I4r+E4KHqVmw1BHAwO9yjGI6F0Y44Olpk59vc3Kik+JefQImDKlRq9m5OVYC+aG2sh4V/M3wO9ELS15n3w1DRp8U36RGHvC+CDCMg0iO3wY+Pln/vPGjdU2599QRxMrBzeDhlCAIxGJ2PHqy4XCdOjAfy8BviTnG9UTCalOlKhVSDN7EwgFwOOXuW/PGRUK+R8PAChWyU3RNpx/CAD4zNtevvBKZTx+DHTvzhPRhQuvt3fqxEcH13AioQDdpc3fJdX+lrK05Eswzp2rtJaFk1EpeJFdAEsjbbRv+J5iMvHxvHIeAHz9NR9DUY087Ewwzb8hAGDeodu49+bYjsr68Uc+6+DZM95F82HNZCUqhBK1CjHU0URD6QId76r7XUXLXt56nIarj15AQyjA8FaOlTvYw4evf7az4wOMvvmmTFWqaiLZ6O/b72j+BuRbFpRwVy1dzvITTztovKuuu7Rf+sULPl/6l1+qKUJ5o9o4oY2zGfIKJZiw7QbyChU4K0Jbm9cC19ICjhyhFbaI0lCiVjGvm79L6KeWJuqLF/lCBwomXXyjp5s1rI3frstcJhERvIm+fn3+s9RvvwHLlwMWFpUPVA2Vuflb6soVPi3v6NGqD+6VhBc5uPAqtvd2e2zbBly6BBgbV0u/dGmEQgGWDXSHmYEWYpIz8eMRBS+64+rKp5yZmwM2Noo9NiFlRIlaxUhX0ipx2omzM2Bry9cyvnhRoed9mpYra5b9so1T+Q9w9y5fJMPdnRcoEQiAs2cVGqM6ExUf/f2u5m+p3buB8HA+JqGamlx3XU8AY0Dr+mawr13CmIGsLP67BwBDhwIrV/IiIUpaJ13KwlAHywd6AAC2XI5HYKSCSw5PnsxXAevVS7HHJaSMKFGrGOlKWrefZiCn4I2RvwJBlTV/b74YC7GE4SOnWuVbACE2FvjiC6BRI16ghDE+hzYyUm3WW64uZW7+BoA5c4BJk3gRkWoo9lIkluC/648B8LnTbwkI4K0h0jt8gYD3S/ftW+WxlUXbBub4qi3/gvnt7lt4kqbAFiehEDAze/08J0dxxy4riQTIzub95fHxVTrzg6geStQqpo6JLqyM+AId4SUt0FEFiTorvwjbr/C+yVFluZvOyOCjfb/6ivc5S0ub9uzJK4n99x9P3EROC8daMDN41fz94D3N3wYGwIoV1dZVcO7eMyRl5MFUTxNdGpnzMqDFp4iJRLy75eTJaomnIqZ0aQh3W2Nk5BVh0vYbKKqKNa137QIcHeWrluXn8776x495y1J4OG/xOnmSz2zYuZP/P7J6Ne/LnzeP931LMQb06cNrjb8o1uU1axafvqiry6+/gQH/fXBw4M3wEyfyVeRokFuNVzOqStQgAoEAno6mOHIrEaGxL9Gqnpn8DtKR39eu8YRpZFTpc+68loDM/CI4meujQ8NSEkNyMu9jPn2ary9cfKCTnx+wYAHw0UeVjqUmkzZ//3s5DkdvJZZ+rUty5gzQtm2VVWrbcSUeLimPMDvjBrTrf8krjR0/zhcLAfhgwIEDVXrFMi0NIVYNbo7uK8/jetxLrAy+h4AuDRV7kv37+V3tr7/yBAzw6VvFE29ZdO/OK58BvHUiKIh/EcrIeL0+tvQLwJtEIiA1ldf/X7WKFw4aOpTPw1ehhYaI4lCiVkFeDjxRlzif2t6eD9S6fx84d47fxVZCkViCTSF8ENnI1nV5ucicHF5cRSR6fQcvEr2eLwvw+bIdOvA/Du3bVyqGD0kPN2v8ezkOJ6KSsVAsgea7RlZL/e9/wNatfJ7yq3XaFSY2Flmb/sXUNX+hQWrc6+1GRjxZSzk68oeKs6+th5/6NcWkHeFYdfo+fOqZwadeGWuVl8Xq1XyAmXSONfB6pL6mJv/5zYeu7tvb3Nzkj7tuHf9/TJqkAd43PmLE28eSSPgUzb//5ivKRUUB333Hl4T96Sf+X1KjUKJWQV6vKpSFxb+ERMLerrXcsSNP1KdOVTpRH7+djGfP0mGtI0L/Zq++jf/7LzBmDNCu3etEbWYGzJzJm7o7dOBTrki5SZu/U7PyEXI/Fe3Lclft4cET9bRpfECTmdl73/JOz57xJtxt24CQEBgAaACgUEMTmr178VrX3bvzpKCG+njUwYV7qdgV+hjf7AzHsUltYKqvoHnpJiY8KRa3ciXw+++Vq8M/dOjb22xsSh5pLhQC/v78kZ7OBx7+8w//4t6s2HKksbG8oFC7dmpfM/9DR/96KqiR9asFOvKKcDelhCIOle2nzs/nfZDz58N+QA/cWjEIC9LDoKsl4q936MCb0Bo0kO//+vFH/geFknSFyY3+vlXG0cmTJvE7sOfPgW+/rdiJs7J4su/Rg//xHz8eCAkBEwgQWr8ZpnWbiMMnwoA9e3jhEjVN0lLz+jSBk7k+kjLyMG33TcWWGH2Ttrby1q42NubFWM6e5bXJ/fxev7Z2Lf9bMXq0cmIjCkOJWgVpiITwsDMBUErhE2k/9c2bvK/qfQoL+eCWn37iA1ZMTXl/55w5cL13A9riQvhmFGv2dHbmI0v/+ENtl5dUZa+XvkxGYVkGPGlq8qZRgA9KOneu/Cddt443oR89ygeJeXoCy5bh+oVb+PjjBTjWojv8fV3Kf1wVpaelgVWDm0FLJMTJ6BT8fTFW2SFVPScn+cVsNDV5F0aPHq+3xcXx34WXyl34h5QPJWoVJS18UuJ8agsLfmc7a1bJ1auKioCrV4ElS4CuXXli9vXliyecPMkHrVhY4Lp3Z8zsMg6/rNgP3c0bX79fIKAEXYVa1uXN3+m5hWUrfgIAPj6v74zGjHk9n7kk168DY8fyu2OpTz/lLSSzZwPR0XyfgABsiefTfHp72EBPq2b1hDWxMcb33fmXj4VH7+D203QlR1TNfvyRL51avHts0yb+u2FlxadRHj5MU73UACVqFeXpyPupS6xQBvCBJPPn86QtFssn7ClTAG9v3pd2/Diff1m7NtC/Px8lGhmJ+NsPMbDDJGxt1h29B3WkxFyNREIBuja1BAAcLUvxE6nFi3mFrOhovuJYccWbdo8c4XdNGza83mZrC9y5w6cGufDklZZTgGORfOnNwS3sK/RZVN2wVo7wa2SJArEEX2+/8XZtgppOV1e+Wb5uXT4YrqCA92336sV/N775hk+tpKleKokStYpqZm8CgQBIeJGLlDcX6Cjuyy/54KJLl15va92aD3rp04fPxb15E0hJ4XdYEyYATZpg48VYSBgvFNHQquYvjKFqyt38DfCWkeXL+c8LFvB+ycWLef/13r2v9xsyBBg+nA8+K+6NL2P7bjxBQZEEja2N0LRO5af5qSKBQIBfBrjBykgHD59lY86B28oOSbmGDQNu3eJJ+Ztv+Bf9lBT+d6J5c/67tHQpkKjg6m6kUihRqygjHU00tOQJtNRlLwE+7zItjc+zlerbl/dd79//eiBSsVGf6TmF+O86n3rzZWvlln/8UHnXrQ0zAy2k5RTi4oPnZX/jkCF8jEJeHp8WN2MGr6m+ffvrferX502cnTqVehjGGHZc5b8Dn7a0g6AGt6iY6mthxaceEAqAXaGPcSD8ibJDUj4PD/6l7/Fj3vz9ySd88ZHISP4Fz9YW6NaN/14poxIbkUOJWoVJ636XOKBMasYM3h89ffrrbZqafE5mKbZfi0dOgRgNLQ3RxrmSU31IhfDmbz76+2hZR38D/K547VpAX5//3KEDb+Iu3sxdBuEJaYhJzoS2hhB9POqU673q6COn2pjQ0RkAMHNfJOKeZys5IhWhqckHm/33H+/PXrcOaNWKd6UFBvKpeqGhyo7yg0eJWoVJ51OHltZPDfB5ky1ayI/2fIeCIgk2h8QCAEa2qVuj76RUnbT5+3hUUtmbvwGgYUPg3j3g6VM+Re/LL3mzeDnsvMbvpnu4WsNYV0lTi6rZxI710dKxFrLyizBx+43311v/0Jia8rLAISG8FOqsWfyLoK/v630WLuTrpaekKC3MDxElahVWfIGO3ALFrLN7NCIRSRl5MDfURh8PWrZPmSrc/A0A1tZ85G4FZOUX4eDNpwCAT1vWzEFkJdEQCbHiUw8Y62ri5uN0LDsRo+yQVJezMx+seurU626z/Hxg2TI+IFHBiwKRd6NErcJsTXVhaaSNotIW6Cgnxhg2nH8IABjm4wBtjdKbx0nVEwkF8G9SgebvSjp88ylyCsRwMtNHC8fy3YmrOxsTXfw8gJfvXH/uIc7efabkiNSIQMBnjXzxBa/7LhUVJb+AC1E4StQqTCAQlK35u4wuP3yB208zoKMpxBBvh0ofj1Rej4o2f1fC9lfN3oNa1OxBZKXxb2KFzz/iv/9T/gtHSuY7ZlWQ17S0eJ/1X3+9vsvOyuLVz5o25YNXaXpXlaBEreKkzd/vHPldRn++upse4GmruNrHpFJa1q2F2vq8+ftSeZu/KyA6MQM3E9KgIRTgY88Pd6WlmT0awcXKEKlZBZjy301IJJRgKuT2bV7HISYG6NePTw29eFHZUdU4KpGoV69eDUdHR+jo6MDb2xtXr14tdd8NGzagTZs2MDU1hampKfz8/N65v7qTJuqwuJeV+mPy4FkWgu+kQCAAvvClKVmqQkMkhL909Hd5ip9UkHQQWefGljAz0K7y86kqHU0RVg1uBh1NIc7fS5V1CZFy8vbmNcZnzuTFVS5e5IPP+vfnyZsohNIT9c6dOxEQEIA5c+YgLCwM7u7u8Pf3R0opowrPnDmDwYMH4/Tp07h06RLs7OzQpUsXPHlSM+dGNrYxgq6mCBl5RbiXklXh4/x1gS9l2cnFEk7mBooKjyiArPn7dtU2f+cVirHvBv//5EMaRFYaZ0tDzOnVBADwy/EYhYwD+SAZGfFypffvA6NG8WbxffuAJk14udKkJGVHqPaUnqiXL1+OUaNGYcSIEWjcuDHWrVsHPT09bNy4scT9t27dinHjxsHDwwMuLi74888/IZFIEBwcXM2RVw9NkRDudsYASqn7XQYvsguwJ/QxAODLNnQ3rWq8XzV/v6zi5u/jt5OQnluIOia6aF2f5s8DwKct7NDD1RpFEoaJ228gM4/qXleYjQ1fyCciAujdmzeJr1vHC/DMmQNklrASICkTpSbqgoIChIaGwq/Y0mxCoRB+fn64VLwk5jvk5OSgsLAQtYovuF5Mfn4+MjIyZI9MNfxlkQ4oK7Xu93tsuRyH/CIJXOsYw7tuydeJKE91NX9LK5F94mUL0ZtrnH+gBAIBFvZ3RR0TXcS/yMHMfZFVuyTmh6BxY+DAAb7Km7c3X2tg/nyesNesoUVAKkCpiTo1NRVisRiWlpZy2y0tLZFUxuaS6dOnw8bGRi7ZF7do0SIYGxvLHo0bN6503NXN0/EdK2m9R16hGP9cigXA76Y/xFG+6qCqm79jU7Nx6eFzCATAQC9aT7w4Y11NrBzcDCKhAAdvPsWuV61PpJLatOFrEOzaxedlp6TwYil5NMq+vJTe9F0Zixcvxo4dO7Bv3z7o6OiUuM+MGTOQnp4ue0RFRVVzlJXX3N4UAgEQ9zwHzzLzy/XegzefIjWrANbGOrJKWET1eNethVqvmr8vP1R88/fOV7Xd2zUwh42JrsKPr+48HUwR0LkBAGDOgdu4X4nxIKQYgQAYMICPDl+9GvjlF8Dw1SJAjPHlVlUZY3yw3JYtSp16ptREbWZmBpFIhOTkZLntycnJsHpP1aWlS5di8eLFOHHiBNzc3ErdT1tbG0ZGRrKHoaH6rRRlrKuJBhY87vLMp2aM4a/zfBDZ8FaO0BSp9feyGk1DJHxd/ETBzd+FYgl2Xed3iZ/W0OUsFWFsu3rwrV8buYVifL39BvIKFVMNkIDXFB83jq/eJbVnDy9//NlnyovrTYwBz4t9Uc7P5035n3/OB8spiVL/cmtpacHT01NuIJh0YJiPj0+p7/v555+xYMECBAYGwsvLqzpCVTrPsizQ8Ybz91IRk5wJfS0RjfJVA6+bv5NRpMDm71N3UpCalQ8zA210amShsOPWNEKhAL8O9EBtfS1EJ2Zg8bE7yg6pZouJ4YsHNWigvBgY43f7a9YAgwbxAXHFV53T0QHatuVTztLTlRZm2VZyqEIBAQEYNmwYvLy80LJlS6xYsQLZ2dkYMWIEAGDo0KGoU6cOFi1aBABYsmQJZs+ejW3btsHR0VHWl21gYAADg5o77cjLwRTbrsSXq/CJdG7owBZ2H8zCC+rsIyfe/P0iuwCXH75AawWtbCadOz3A05ZaVd7DwkgHSwe6Y8Sma9h8MRa+9c3QubHl+99Iym/mTN4sblNszYGzZ4ETJ4BvvwWMjRV/TomEj0o/e5Y/zp3jSwIXl57OB8Dp6/Pnx4/LLROsDEr/v3bQoEFYunQpZs+eDQ8PD4SHhyMwMFA2wCw+Ph6JxRYxX7t2LQoKCjBgwABYW1vLHkuXLlXWR6gW0pHft5+ml6lJLiYpE+fvpUJIBU7URvHm7yMRTxVyzMT0XJyJ4TUJBrWgQWRl0aGhhWyd9mm7byIxPVfJEdVgDRu+7rOWSICpU/kKXfXqAStXAgUFlTt+URHvB1+2jE8Zq12br8U9aRKwdy9P0rq6/C56/nyevF+8eJ2kAaUnaUAF7qgBYMKECZgwYUKJr505c0bueWxsbNUHpILsaunC3FAbzzLzcTMhDd5Otd+5v7RcaNemVrCrpVcdIRIF6OFqje1X43H8djIW9JFAo5J3wLuuP4aE8cFqdc303/8GAgD4tqsLrjx6gYgn6Zi8IxzbRn1EU9qqmkDA77K/+443i0+aBPz2G/DTT3wRkLIkzMJCICODJ2SAN2u3aCG/j4EBb8pu144/vLx4HXMVpvyvCqRM+AIdZav7nZKZhwPh/I7syzZOVR4bUZw3m78rQyJhsmbvwTRGoVy0NIRYNbgZ9LVEuPLoBX4/pbyBRB8MgQDo2xeIjOSFUqysgIcPgcGD+Xzs06ff/f5Nm/ia2t9883pb06aAoyPQsyfw88/AlSvAy5dAYCAwYwbQqpXKJ2mAErVakdb9ft986n8vxaFALEFzexM0t/+wljFUd7z5m3f7HKnk6O8L91PxJC0XRjoa6Nq0YmtXf8gczfTxY7+mAIDfgu/iWmzlV7AjZaChAXz1FXDvHm+ONjDgzdcdOwLdu/NVuubN488DA1+/z9aW9y1HRLzeJhLxZH/oEDBtGtCyJT++mqFErUa8HKVLXpa+QEdugRhbLscBAEbR3bRa6l6s+EllRn9L76b7N7eFjiatPV4R/ZrZon/zOpAwYNL2G0jLqWSfKSk7AwNg1iw+j3n8eJ5gjx3jq3TNncvvsE+efL1/69bArVtAaKj8cWpAkSdK1GqkiY0RdDSFSM8txINnJRdk2BP2GC9zCmFXSxddmtBdlDrycaoNUz1NvMguwJVHFbuLe56VjxNRfEYEDSKrnAV9mqKumT6epudh+p5bVGK0ullYAL//DkRF8SlUzs7Ap5/y5vGxY1/vp6sLuLqqxOAvRat5n6gG0xQJ4W5rAqDkfmqJhGHjq1WyvvCtS4Nf1JT86O+KNX/vDXuCQjGDu60xGlkbKTK8D46+tgZWDW4GTZEAx28nY8uVeGWH9GFydgZ27ADu3gW2b+fN4/XqKTuqakGJWs14vaPwyak7KXiYmg1DHQ18QvWc1Zqs+Tuy/M3fjDFsv8aTCRW6UYymdYzxXbdGAIAFh6NwJylDyRGRDwklajUjnU9dUinRPy/wKVmfedvDQFv9BkyQ13zq8ebv5xVo/r4e9xIPn2VDT0uEXu42738DKZMvfB3R0cUCBUUSjNsahkep2coOiXwgKFGrGeko7tg3FuiIfJKOyw9fQEMowPBWjkqKjiiKZiWav6XLWfZys6EvbAokEAjwywA3WBnp4OGzbHT/7Ty2XomjPmtS5ShRqxljPU00sOSlUotP05IWOOnpZg1rY1odqSaoSPN3em6hrKrZoJbU/aFotQ20sXdcK7SqxxfvmLkvEl9svoaUTFq6kVQdStRqyPON5u/E9FwcvsXvuqjASc3hU682TF41f18tY/P3wZtPkVcoQQNLAzSzM6naAD9QNia62DLSG7N6NoaWhhCnY57B/9dzCIxU7KpnquhFdgF2XU+gLybVjBK1GnqzQtnmi7EokjB85FQLTetUQSF7ohSaIiH8G5ev+XvH1VeDyFrYQ1AD5o+qKqFQgJGt6+Lw163R2NoIL3MKMWZLGKbuuonMvEJlh6dwRWIJ/r4Yi/a/nMa03bfQ4ZczWH36Pi0FWk0oUash6cjvyCfpeJ6Vj22vpot82Zrupmua7m6vi5+ISylyIxX5JB23n2ZASyREv2Z1qiO8D14DS0PsH++Lse3rQSAAdoc+RtcV53Hl4fP3v1lNXHrwHD1XXcCcg7eRkVcEQx0NZBeI8cvxGHRadhZHbiVSP30Vo0Sthuxr6cHMQBuFYoZZByKRmVcEJzN9dHShtYZrmlavmr9Tswpw5dG7//jveDUlq2tTK5jqq3794ppCS0OI6V1d8N9XPrA11cWTtFx8uuEyFh2LRn6R+t5xPk3LxfhtYRi84TLuJGXCRE8TC/o2xY1ZnbFikAesjHTw5NU+A9dfwq3HacoOucaiRK2Gii/QcTSCV5/6onVdCKnASY2jKRKiy6v1kI/cKr35O6egCAdu8EFkn1IlMqVo4VgLxya1wUAvWzAGrD/7EH1+D1G7Odd5hWL8fuqe7G5ZKAA+/8gBp6e0x+cfOUBDJETfZnVwamo7TPZzho6mENdiX6L37yGY8t9NJGdQ/7WiUaJWU9LmbwAw1dPEx81tlRgNqUo93Phc6Hc1fx+NSEJmfhEcauvho/csgUqqjqGOJn4e4I71n3uilr4W7iRloveqEGw497DU+vyqgjGGE7eT0PnXs1h64i5yC8Vo6VgLh75ujQV9m77VSqOnpYHJfg1wemp7WVfLnrDH6LD0DFYF36P+awWiRK2mpCtpAcD/PnKArhYtulBTlaX5WzqIbKCXHbWsqAD/JlY4PrktOrlYoEAswU9Ho/HZn5fxJC1X2aGV6H5KFoZtuobR/4Yi4UUurIx08NunHtj51UdoYvPuAarWxrr4dZAH9o1rhWb2JsgpEGNZ0F10WnYWB28+pf5rBaBEraaa2BjD0kgbhjoa+NzHQdnhkCpUvPn7aAmjv++nZOJ63EuIhAJ84kktK6rC3FAbfw7zwqL+rtDTEuHywxfo+us57LvxWGWSV2ZeIRYejUbXFedw7u4zaImEGNe+HoKntEMfjzrlmjnQzN4Ue8e2wsrBzWBjzPuvJ26/gY/XXkR4QlrVfYgPACVqNaWlIcTBCa1xfHJbWBjqKDscUsWkxU8CI5Pfav6WLmfZ0cUCFkb0u6BKBAIBBre0x9GJbdDc3gSZ+UX4ZudNTNh2Ay+zlbdkpkTCsCf0MTouO4s/zj1EkYShk4sFTnzTFt92dYF+BSvaCQQC9Ha3QfCU9pjSuQF0NUUIi09D39UhCNgZjsR01WxRUHWUqNWYpZEObEyoCtmHwLe+GYx1NZGalS9X/CS/SIw9YU8A0CAyVeZopo//vvLB1C4NoCEU4EhEIvxXnMPZu8+qPZaIx+kYsO4ipuy6iWeZ+ahrpo9Nw1vgr+Et4Gimr5Bz6GqJ8HUnZ5ye2l42fmbvjSfouPQsVpy8i9wC9e2/ft80yapAiZoQNVBa8/fJqBS8yC6AlZEO2jUwV1Z4pAw0REJM6OiMfeN8Uc9cHymZ+Ri28SpmH4islsT1PCsfM/beQu/VFxAWnwZ9LRG+6+aCwMlt0KGKpnZaGetg2UB3HBjvCy8HU+QWirHi5D10XHYG+288UfkBdgAv9hIW/xKrgu9h4PpL6LriXLXHQBX7CVET3d2ssSv0MY5FJmFu7yYQCQWyudOfeNlCQ0Tfu9WBq60xjkxsg8XH7mDzxVj8cykOF+6lYvkgD3hUQdnXIrEE/16Ow/Kgu8jMKwIA9GtWB991c4FlNXWVuNuZYNcYHxyJSMSio3fwJC0Xk3eGY/PFWMzu1Vi22JAqYIzhUWo2LtxPxYV7qbj08Lnsukk9SctFnWpszaRETYia8K0n3/xta6qL8/dSAfDR3kR96GiKMLd3E3RqZIGpu27iYWo2Pl57ERM7OmN8h3oK+9J18UEq5h2MQkxyJgCgiY0R5vVuAi/HWgo5fnkIBAL0dLOBXyNL/HXhEVafvo/whDT0X3MRfTxsML2ri9K68lKz8hFyPxUhr5Lz03T5ueDGuppoVa82fOuboY2zGWyMq3csCCVqQtSElgZv/t4V+hhHIxJhqqcJAGjjbAa7WnpKjo5URBtncxyf3BY/7I/E4VuJ+PXkXZyKScGvA93hZG5Q4eM+ScvFwiPRshrxpnqamOrfEJ+2sIdIydP3dDRFGN+hPj7xtMXSEzHYFfoYB8Kf4vjtJIxuWw9j2jlBT6tqU1NugRhXY1/gwr1nuHD/OaIT5YvSaImE8HQwRWtnM7Sub4amdYyVet0ETFXmCVSTx48fw87ODgkJCbC1paksRL2cjknBiE3XYGagDQ2hAEkZefj9s2bo+aooClFfB8Kf4If9vCSwrqYIM3s0whDv8i2uklcoxh/nHmLNmfvIK5RAKOB1FgI6N4CJnmqWlY18ko75h6JwNZYPkrQ00sb0ri7o61FHYTUBxBKGiCfpCLmfivP3niEsLg0Fbywd28jaCG2czeBb3wwtHWtVeW2K8uQiStSEqJGCIgm8fgxCxqs+s1r6Wrg0oyO0NajgTU3wNC0XU3fdxMUHvLBN+4bm+Pljt/dOu2OM4URUMn48EoWEF3wKVMu6tTCvdxM0sjaq8rgrizGGwMgkLDwWLYvf3dYYs3s1li3rW97jxT3Pwfn7qQi5l4qLD1Jl/89I2Rjr8DtmZ3O0qlcbZgbaCvksZUWJ+h0oURN1N3XXTewOfQwA+LJ1XfzQs7GSIyKKJJEwbLoYiyWBd1BQJIGpniYW9XdF16bWJe5/PyUL8w7dlo1XsDbWwffdG6Gnm7XaLXWaVyjGppBYrD59H1n5PLH2dLPGd91cYGv67u6d51n5uPjg+au75tS3qsAZ6migVb3aaF2f3zXXNdNX6vVRu0S9evVq/PLLL0hKSoK7uztWrVqFli1blrr/rl27MGvWLMTGxsLZ2RlLlixB9+7dy3QuStRE3Z2+k4IRm68BAE4GtEV9C0MlR0Sqwt3kTEzeEY6oV/2nHze3xZzejWGkw8cmZOYVYmXwPWwK4evRa4mEGN3WCeM61KvyPt6qlpKZh+Un7mLn9QQwxsdnjG7jhLHt68mKseQVinH10Qs+AOx+Km4/le9n1hQJ0NzeFK3rm6G1sxlc6xir1MwItUrUO3fuxNChQ7Fu3Tp4e3tjxYoV2LVrF2JiYmBh8fbcvosXL6Jt27ZYtGgRevbsiW3btmHJkiUICwtD06ZN33s+StRE3RWKJZi++xYsjHTwXTcXZYdDqlBBkQQrTt7FurMPIGFAHRNdLP3EHY9f5mBJYAxSs/IBAH6NLDGrZyM41FZMwRJVcftpOhYcjsLlh7z/2txQGx83t8Wtx2m4HvcSBUXy/cwuVob8jtmZ9zNXtMJadVCrRO3t7Y0WLVrg999/BwBIJBLY2dnh66+/xnfffffW/oMGDUJ2djYOHz4s2/bRRx/Bw8MD69ate+/5KFETQtTNtdgXCPgvXNZ/K+Vkpo/ZvRqjfcOauxa9tP994dFoxD3PkXvNykhHNjK7Vf3aalVOuTy5SKlfNwoKChAaGooZM2bItgmFQvj5+eHSpUslvufSpUsICAiQ2+bv74/9+/dXZaiEEKI0fK3rtlhwKAo7rydAX0uEiZ2cMcK3LrQ0VKc5tyoIBAL4N7FC+4bm2HI5HhGP09DM3hS+9c1Qz1y5/czVRamJOjU1FWKxGJaWlnLbLS0tcefOnRLfk5SUVOL+SUlJJe6fn5+P/Px82fPMzMxKRk0IIdXPQFsDSwa4YbivIywMtVG7mkcpK5u2hggjW9dVdhhKUbO/igFYtGgRjI2NZY/GjWmELCFEfTWyNvrgkvSHTqmJ2szMDCKRCMnJyXLbk5OTYWVlVeJ7rKysyrX/jBkzkJ6eLntERUUpJnhCCCGkGig1UWtpacHT0xPBwcGybRKJBMHBwfDx8SnxPT4+PnL7A0BQUFCp+2tra8PIyEj2MDSkqSyEEELUh9LHrgcEBGDYsGHw8vJCy5YtsWLFCmRnZ2PEiBEAgKFDh6JOnTpYtGgRAGDSpElo164dli1bhh49emDHjh24fv06/vjjD2V+DEIIIaRKKD1RDxo0CM+ePcPs2bORlJQEDw8PBAYGygaMxcfHQyh8fePfqlUrbNu2DT/88AO+//57ODs7Y//+/WWaQ00IIYSoG6XPo65uNI+aEEKIsqnNPGplkEh4JZvExEQlR0IIIeRDJc1B0pz0Lh9copaOGH9XLXFCCCGkOiQnJ8Pe3v6d+3xwTd9FRUW4ceMGLC0t5fq+KyIzMxONGzdGVFQUjSYvA7pe5UfXrHzoepUPXa/yUeT1kkgkSE5ORrNmzaCh8e575g8uUStSRkYGjI2NkZ6eDiMj1V/zVdnoepUfXbPyoetVPnS9ykdZ16vGVyYjhBBC1BklakIIIUSFUaKuBG1tbcyZMwfa2lR3tyzoepUfXbPyoetVPnS9ykdZ14v6qAkhhBAVRnfUhBBCiAqjRE0IIYSoMErUhBBCiAqjRF0Jq1evhqOjI3R0dODt7Y2rV68qOySVde7cOfTq1Qs2NjYQCATYv3+/skNSWYsWLUKLFi1gaGgICwsL9O3bFzExMcoOS2WtXbsWbm5usqVsfXx8cOzYMWWHpTYWL14MgUCAyZMnKzsUlTV37lwIBAK5h4uLS7WdnxJ1Be3cuRMBAQGYM2cOwsLC4O7uDn9/f6SkpCg7NJWUnZ0Nd3d3rF69WtmhqLyzZ89i/PjxuHz5MoKCglBYWIguXbogOztb2aGpJFtbWyxevBihoaG4fv06OnbsiD59+uD27dvKDk3lXbt2DevXr4ebm5uyQ1F5TZo0QWJiouxx4cKF6js5IxXSsmVLNn78eNlzsVjMbGxs2KJFi5QYlXoAwPbt26fsMNRGSkoKA8DOnj2r7FDUhqmpKfvzzz+VHYZKy8zMZM7OziwoKIi1a9eOTZo0Sdkhqaw5c+Ywd3d3pZ2f7qgroKCgAKGhofDz85NtEwqF8PPzw6VLl5QYGamJ0tPTAQC1atVSciSqTywWY8eOHcjOzoaPj4+yw1Fp48ePR48ePeT+jpHS3bt3DzY2NnBycsKQIUMQHx9fbef+4FbPUoTU1FSIxWJYWlrKbbe0tMSdO3eUFBWpiSQSCSZPngxfX180bdpU2eGorIiICPj4+CAvLw8GBgbYt28fGjdurOywVNaOHTsQFhaGa9euKTsUteDt7Y3NmzejYcOGSExMxLx589CmTRtERkZWy2ImlKgJUWHjx49HZGRk9faHqaGGDRsiPDwc6enp2L17N4YNG4azZ89Ssi5BQkICJk2ahKCgIOjo6Cg7HLXQrVs32c9ubm7w9vaGg4MD/vvvP4wcObLKz0+JugLMzMwgEolka1tLJScnw8rKSklRkZpmwoQJOHz4MM6dOwdbW1tlh6PStLS0UL9+fQCAp6cnrl27ht9++w3r169XcmSqJzQ0FCkpKWjevLlsm1gsxrlz5/D7778jPz8fIpFIiRGqPhMTEzRo0AD379+vlvNRH3UFaGlpwdPTE8HBwbJtEokEwcHB1C9GKo0xhgkTJmDfvn04deoU6tatq+yQ1I5EIkF+fr6yw1BJnTp1QkREBMLDw2UPLy8vDBkyBOHh4ZSkyyArKwsPHjyAtbV1tZyP7qgrKCAgAMOGDYOXlxdatmyJFStWIDs7GyNGjFB2aCopKytL7tvno0ePEB4ejlq1asHe3l6Jkame8ePHY9u2bThw4AAMDQ2RlJQEADA2Noaurq6So1M9M2bMQLdu3WBvb4/MzExs27YNZ86cwfHjx5UdmkoyNDR8a7yDvr4+ateuTeMgSjF16lT06tULDg4OePr0KebMmQORSITBgwdXy/kpUVfQoEGD8OzZM8yePRtJSUnw8PBAYGDgWwPMCHf9+nV06NBB9jwgIAAAMGzYMGzevFlJUammtWvXAgDat28vt33Tpk0YPnx49Qek4lJSUjB06FAkJibC2NgYbm5uOH78ODp37qzs0EgN8fjxYwwePBjPnz+Hubk5WrdujcuXL8Pc3Lxazk+rZxFCCCEqjPqoCSGEEBVGiZoQQghRYZSoCSGEEBVGiZoQQghRYZSoCSGEEBVGiZoQQghRYZSoCSGEEBVGiZoQQghRYZSoCSFVRiAQYP/+/coOgxC1RomakBpq+PDhEAgEbz26du2q7NAIIeVAtb4JqcG6du2KTZs2yW3T1tZWUjSEkIqgO2pCajBtbW1YWVnJPUxNTQHwZum1a9eiW7du0NXVhZOTE3bv3i33/oiICHTs2BG6urqoXbs2Ro8ejaysLLl9Nm7ciCZNmkBbWxvW1taYMGGC3Oupqano168f9PT04OzsjIMHD8pee/nyJYYMGQJzc3Po6urC2dn5rS8WhHzoKFET8gGbNWsWPv74Y9y8eRNDhgzBp59+iujoaABAdnY2/P39YWpqimvXrmHXrl04efKkXCJeu3Ytxo8fj9GjRyMiIgIHDx5E/fr15c4xb948DBw4ELdu3UL37t0xZMgQvHjxQnb+qKgoHDt2DNHR0Vi7di3MzMyq7wIQog4YIaRGGjZsGBOJRExfX1/u8dNPPzHGGAPAxowZI/ceb29vNnbsWMYYY3/88QczNTVlWVlZstePHDnChEIhS0pKYowxZmNjw2bOnFlqDADYDz/8IHuelZXFALBjx44xxhjr1asXGzFihGI+MCE1FPVRE1KDdejQQba+tVStWrVkP/v4+Mi95uPjg/DwcABAdHQ03N3doa+vL3vd19cXEokEMTExEAgEePr0KTp16vTOGNzc3GQ/6+vrw8jICCkpKQCAsWPH4uOPP0ZYWBi6dOmCvn37olWrVhX6rITUVJSoCanB9PX132qKVhRdXd0y7aepqSn3XCAQQCKRAAC6deuGuLg4HD16FEFBQejUqRPGjx+PpUuXKjxeQtQV9VET8gG7fPnyW88bNWoEAGjUqBFu3ryJ7Oxs2eshISEQCoVo2LAhDA0N4ejoiODg4ErFYG5ujmHDhmHLli1YsWIF/vjjj0odj5Cahu6oCanB8vPzkZSUJLdNQ0NDNmBr165d8PLyQuvWrbF161ZcvXoVf/31FwBgyJAhmDNnDoYNG4a5c+fi2bNn+Prrr/H555/D0tISADB37lyMGTMGFhYW6NatGzIzMxESEoKvv/66TPHNnj0bnp6eaNKkCfLz83H48GHZFwVCCEeJmpAaLDAwENbW1nLbGjZsiDt37gDgI7J37NiBcePGwdraGtu3b0fjxo0BAHp6ejh+/DgmTZqEFi1aQE9PDx9//DGWL18uO9awYcOQl5eHX3/9FVOnToWZmRkGDBhQ5vi0tLQwY8YMxMbGQldXF23atMGOHTsU8MkJqTkEjDGm7CAIIdVPIBBg37596Nu3r7JDIYS8A/VRE0IIISqMEjUhhBCiwqiPmpAPFPV6EaIe6I6aEEIIUWGUqAkhhBAVRomaEEIIUWGUqAkhhBAVRomaEEIIUWGUqAkhhBAVRomaEEIIUWGUqAkhhBAVRomaEEIIUWH/Bx/n85YhW4lqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Chapter05 import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_losses(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate the accuracies on the full datasets (during training we approximated the training and validation set accuracies from 5 batches via the eval_iter=5 setting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
