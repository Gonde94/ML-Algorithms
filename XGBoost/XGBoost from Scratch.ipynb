{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd0c8e8",
   "metadata": {},
   "source": [
    "## XGBoost from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9be0658",
   "metadata": {},
   "source": [
    "Taken from https://blog.mattbowers.dev/xgboost-from-scratch. This notebook is to help demystify how XGBoost works under the hood for myself. This implementation will include many key hyperparameters and their functionality, as well as support user-defined custom objective functions (regression, classification, etc.) All that is needed for this is a twice-differentiable objective function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8a84e1",
   "metadata": {},
   "source": [
    "### The XGBModel Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fc84b",
   "metadata": {},
   "source": [
    "This is the user-facing API for the model, a class called XGBoostModel. This will implement gradient boosting and prediction. \n",
    "\n",
    "\"To be more consistent with the XGBoost library, we'll pass hyperparameters to our model in a parameter dictionary, so our init method is going to pull relevant parameters out of the dictionary and set them as object attributes. Note the use of python's defaultdict so we don't have to worry about handling key errors if we try to access a parameter that the user didn't set in the dictionary.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0ca7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd94a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostModel():\n",
    "    '''XGBoost from Scratch\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, params, random_seed=None):\n",
    "        self.params = defaultdict(lambda: None, params)\n",
    "        self.subsample = self.params['subsample'] \\\n",
    "            if self.params['subsample'] else 1.0\n",
    "        self.learning_rate = self.params['learning_rate'] \\\n",
    "            if self.params['learning_rate'] else 0.3\n",
    "        self.base_prediction = self.params['base_score'] \\\n",
    "            if self.params['base_score'] else 0.5\n",
    "        self.max_depth = self.params['max_depth'] \\\n",
    "            if self.params['max_depth'] else 6\n",
    "        self.rng = np.random.default_rng(seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb6e0b",
   "metadata": {},
   "source": [
    "\"The fit method, based on our classic GBM, takes a feature dataframe, a target vector, the objective function, and the number of boosting rounds as arguments. The user-supplied objective function should be an object with loss, gradient, and hessian methods, each of which takes a target vector and a prediction vector as input; the loss method should return a scalar loss score, the gradient method should return a vector of gradients, and the hessian method should return a vector of hessians.\n",
    "\n",
    "In contrast to boosting in the classic GBM, instead of computing residuals between the current predictions and the target, we compute gradients and hessians of the loss function with respect to the current predictions, and instead of predicting residuals with a decision tree, we fit a special XGBoost tree booster (which we'll implement in a moment) using the gradients and hessians. I've also added row subsampling by drawing a random subset of instance indices and passing them to the tree booster during each boosting round.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d7c334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, objective, num_boost_round, verbose=False):\n",
    "    current_predictions = self.base_prediction * np.ones(shape=y.shape)\n",
    "    self.boosters = []\n",
    "    for i in range(num_boost_round):\n",
    "        gradients = objective.gradient(y, current_predictions)\n",
    "        hessians = objective.hessian(y, current_predictions)\n",
    "        sample_idxs = None if self.subsample == 1.0 \\\n",
    "            else self.rng.choice(len(y), \n",
    "                                 size=math.floor(self.subsample*len(y)), \n",
    "                                 replace=False)\n",
    "        booster = TreeBooster(X, gradients, hessians, \n",
    "                              self.params, self.max_depth, sample_idxs)\n",
    "        current_predictions += self.learning_rate * booster.predict(X)\n",
    "        self.boosters.append(booster)\n",
    "        if verbose: \n",
    "            print(f'[{i}] train loss = {objective.loss(y, current_predictions)}')\n",
    "            \n",
    "def predict(self, X):\n",
    "    return (self.base_prediction + self.learning_rate \n",
    "            * np.sum([booster.predict(X) for booster in self.boosters], axis=0))\n",
    "\n",
    "XGBoostModel.fit = fit\n",
    "XGBoostModel.predict = predict       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2a9f1",
   "metadata": {},
   "source": [
    "### The XGBoost Tree Booster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59e77c2",
   "metadata": {},
   "source": [
    "The XGBoost tree booster is a modified version of the decision tree that we built in the decision tree from scratch post. Like the decision tree, we recursively build a binary tree structure by finding the best split rule for each node in the tree. The main difference is the criterion for evaluating splits and the way that we define a leaf's predicted value. Instead of being functions of the target values of the instances in each node, the criterion and predicted values are functions of the instance gradients and hessians. Thus we need only make a couple of modifications to our previous decision tree implementation to create the XGBoost tree booster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345ade5",
   "metadata": {},
   "source": [
    "#### Initializing and Inserting Child Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866de00",
   "metadata": {},
   "source": [
    "Most of the init method is just parsing the parameter dictionary to assign parameters as object attributes. The one notable difference from our decision tree is in the way we define the node's predicted value. We define self.value according to equation 5 of the XGBoost paper, a simple function of the gradient and hessian values of the instances in the current node. Of course the init also goes on to build the tree via the maybe insert child nodes method. This method is nearly identical to the one we implemented for our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a290519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeBooster():\n",
    " \n",
    "    def __init__(self, X, g, h, params, max_depth, idxs=None):\n",
    "        self.params = params\n",
    "        self.max_depth = max_depth\n",
    "        assert self.max_depth >= 0, 'max_depth must be nonnegative'\n",
    "        self.min_child_weight = params['min_child_weight'] \\\n",
    "            if params['min_child_weight'] else 1.0\n",
    "        self.reg_lambda = params['reg_lambda'] if params['reg_lambda'] else 1.0\n",
    "        self.gamma = params['gamma'] if params['gamma'] else 0.0\n",
    "        self.colsample_bynode = params['colsample_bynode'] \\\n",
    "            if params['colsample_bynode'] else 1.0\n",
    "        if isinstance(g, pd.Series): g = g.values\n",
    "        if isinstance(h, pd.Series): h = h.values\n",
    "        if idxs is None: idxs = np.arange(len(g))\n",
    "        self.X, self.g, self.h, self.idxs = X, g, h, idxs\n",
    "        self.n, self.c = len(idxs), X.shape[1]\n",
    "        self.value = -g[idxs].sum() / (h[idxs].sum() + self.reg_lambda) # Eq (5)\n",
    "        self.best_score_so_far = 0.\n",
    "        if self.max_depth > 0:\n",
    "            self._maybe_insert_child_nodes()\n",
    "\n",
    "    def _maybe_insert_child_nodes(self):\n",
    "        for i in range(self.c): self._find_better_split(i)\n",
    "        if self.is_leaf: return\n",
    "        x = self.X.values[self.idxs,self.split_feature_idx]\n",
    "        left_idx = np.nonzero(x <= self.threshold)[0]\n",
    "        right_idx = np.nonzero(x > self.threshold)[0]\n",
    "        self.left = TreeBooster(self.X, self.g, self.h, self.params, \n",
    "                                self.max_depth - 1, self.idxs[left_idx])\n",
    "        self.right = TreeBooster(self.X, self.g, self.h, self.params, \n",
    "                                 self.max_depth - 1, self.idxs[right_idx])\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self): return self.best_score_so_far == 0.\n",
    "\n",
    "    def _find_better_split(self, feature_idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0871a149",
   "metadata": {},
   "source": [
    "#### Split Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54835438",
   "metadata": {},
   "source": [
    "Split finding follows the exact same pattern that we used in the decision tree, except we keep track of gradient and hessian stats instead of target value stats, and of course we use the XGBoost gain criterion (equation 7 from the paper) for evaluating splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf257395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_better_split(self, feature_idx):\n",
    "    x = self.X.values[self.idxs, feature_idx]\n",
    "    g, h = self.g[self.idxs], self.h[self.idxs]\n",
    "    sort_idx = np.argsort(x)\n",
    "    sort_g, sort_h, sort_x = g[sort_idx], h[sort_idx], x[sort_idx]\n",
    "    sum_g, sum_h = g.sum(), h.sum()\n",
    "    sum_g_right, sum_h_right = sum_g, sum_h\n",
    "    sum_g_left, sum_h_left = 0., 0.\n",
    "\n",
    "    for i in range(0, self.n - 1):\n",
    "        g_i, h_i, x_i, x_i_next = sort_g[i], sort_h[i], sort_x[i], sort_x[i + 1]\n",
    "        sum_g_left += g_i; sum_g_right -= g_i\n",
    "        sum_h_left += h_i; sum_h_right -= h_i\n",
    "        if sum_h_left < self.min_child_weight or x_i == x_i_next:continue\n",
    "        if sum_h_right < self.min_child_weight: break\n",
    "\n",
    "        gain = 0.5 * ((sum_g_left**2 / (sum_h_left + self.reg_lambda))\n",
    "                        + (sum_g_right**2 / (sum_h_right + self.reg_lambda))\n",
    "                        - (sum_g**2 / (sum_h + self.reg_lambda))\n",
    "                        ) - self.gamma/2 # Eq(7) in the xgboost paper\n",
    "        if gain > self.best_score_so_far: \n",
    "            self.split_feature_idx = feature_idx\n",
    "            self.best_score_so_far = gain\n",
    "            self.threshold = (x_i + x_i_next) / 2\n",
    "            \n",
    "TreeBooster._find_better_split = _find_better_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9936027",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b70548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    return np.array([self._predict_row(row) for i, row in X.iterrows()])\n",
    "\n",
    "def _predict_row(self, row):\n",
    "    if self.is_leaf: \n",
    "        return self.value\n",
    "    child = self.left if row[self.split_feature_idx] <= self.threshold \\\n",
    "        else self.right\n",
    "    return child._predict_row(row)\n",
    "\n",
    "TreeBooster.predict = predict \n",
    "TreeBooster._predict_row = _predict_row "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489d65f",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27735a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbcfbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredErrorObjective():\n",
    "    def loss(self, y, pred): return np.mean((y - pred)**2)\n",
    "    def gradient(self, y, pred): return pred - y\n",
    "    def hessian(self, y, pred): return np.ones(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf68052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.8,\n",
    "    'reg_lambda': 1.5,\n",
    "    'gamma': 0.0,\n",
    "    'min_child_weight': 25,\n",
    "    'base_score': 0.0,\n",
    "    'tree_method': 'exact',\n",
    "}\n",
    "num_boost_round = 50\n",
    "\n",
    "# train the from-scratch XGBoost model\n",
    "model_scratch = XGBoostModel(params, random_seed=42)\n",
    "model_scratch.fit(X_train, y_train, SquaredErrorObjective(), num_boost_round)\n",
    "\n",
    "# train the library XGBoost model\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "model_xgb = xgb.train(params, dtrain, num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47ef5bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch score: 0.262\n",
      "xgboost score: 0.264\n"
     ]
    }
   ],
   "source": [
    "pred_scratch = model_scratch.predict(X_test)\n",
    "pred_xgb = model_xgb.predict(dtest)\n",
    "print(f'scratch score: {round(SquaredErrorObjective().loss(y_test, pred_scratch), 3)}')\n",
    "print(f'xgboost score: {round(SquaredErrorObjective().loss(y_test, pred_xgb), 3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
